%\documentclass[preprint]{aastex}  % USE THIS TO MAKE BIB, THEN FORMAT USING EMULATEAPJ
\documentclass[twocolumn,numberedappendix]{emulateapj} \shorttitle{PSA64}
\shortauthors{Ali, et al.}

\usepackage{amsmath} \usepackage{graphicx} \usepackage[figuresright]{rotating}
%\usepackage{rotating}
\usepackage{natbib}
%\usepackage{pdflscape} \usepackage{lscape}
\citestyle{aa}

\def\b{\mathbf{b}} \def\k{\mathbf{k}} \def\r{\mathbf{r}} \def\q{\mathbf{q}}
\def\b{\mathbf{b}} \def\kp{\mathbf{k}^\prime}
\def\kpp{\mathbf{k}^{\prime\prime}} \def\V{\mathbb{V}} \def\expval#1{\langle #1
\rangle} \def\At{\tilde{A}} \def\Vt{\tilde{V}} \def\Tt{\tilde{T}}
\def\tb{\langle T_b\rangle} \newcommand{\vis}{\mathbf{v}}
\newcommand{\x}{\mathbf{x}} \newcommand{\xhat}{\hat{\mathbf{x}}}
\newcommand{\A}{\mathbf{A}} \newcommand{\N}{\mathbf{N}}
\newcommand{\C}{\mathbf{C}} \newcommand{\Q}{\mathbf{Q}}
\newcommand{\rhat}{\hat{\mathbf{r}}}
\newcommand{\phat}{\hat{\mathbf{p}}}
\newcommand{\qhat}{\hat{\mathbf{q}}}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}


\begin{document}

\title{PAPER-64 Power Spectrum: XXX a better title}

\author{
Zaki S. Ali\altaffilmark{1}, 
Aaron R. Parsons\altaffilmark{1,2}, 
Haoxuan Zheng\altaffilmark{12},
James E. Aguirre\altaffilmark{4},
Richard F. Bradley\altaffilmark{5,6,7},
Gianni Bernardi\altaffilmark{3}, 
Chris L.  Carilli\altaffilmark{8},
Carina T. Cheng\altaffilmark{1},
David R. DeBoer\altaffilmark{2}, 
Matthew R. Dexter\altaffilmark{2},
Steven R. Furlanetto\altaffilmark{XXX},
Daniel C. Jacobs\altaffilmark{9}, 
Pat Klima\altaffilmark{6},
Adrian C. Y. Liu\altaffilmark{1}, 
David H. E. MacMahon\altaffilmark{2},
David F. Moore\altaffilmark{3}
Jonathan C. Pober\altaffilmark{10}, 
Irina I. Stefan\altaffilmark{11},
William P. Walbrugh\altaffilmark{3}
% XXX more on author list
%include Andre Walker, Jasper Horrell, Jasper (yasper), Matthys Maree
}
%\tableofcontents

\altaffiltext{1}{Astronomy Dept., U. California, Berkeley, CA}
\altaffiltext{2}{Radio Astronomy Lab., U. California, Berkeley, CA}
\altaffiltext{3}{Square Kilometer Array, South Africa Project, Cape Town, South Africa}
\altaffiltext{4}{Dept. of Physics and Astronomy, U. Pennsylvania, Philadelphia, PA} 
\altaffiltext{5}{Dept. of Electrical and Computer Engineering, U. Virginia, Charlottesville, VA}
\altaffiltext{6}{National Radio Astronomy Obs., Charlottesville, VA}
\altaffiltext{7}{Dept. of Astronomy, U. Virginia, Charlottesville, VA}
\altaffiltext{8}{National Radio Astronomy Obs., Socorro, NM}
\altaffiltext{9}{School of Earth and Space Exploration, Arizona State U., Tempe, AZ}
\altaffiltext{10}{Physics Dept.  U. Washington, Seattle, WA}
\altaffiltext{11}{Cavendish Lab., Cambridge, UK}
\altaffiltext{12}{Dept. of Physics, Massachusetts Institute of Technology,
Cambridge, MA}

\begin{abstract}
In this paper, we report new limits on 21cm emission from cosmic reionization based on
a 6-month [XXX] observing campaign with a 64-element deployment of the Donald C. Backer
Precision Array for Probing the Epoch of Reionization (PAPER) in South Africa.  This
work extends the work presented in \citet{parsons_et_al2014} with more collecting area,
improved redundancy-based calibration, optimal fringe-rate filtering, and improved
power-spectral analysis using optimal quadratic estimators.  The result is a new
$2\sigma$ upper limit on $\Delta^2(k)$ of $(XXX {\rm mK})^2$ in the range
$0.1<k<0.5\ h\ {\rm Mpc}^{-1}$ at $z=XXX$.  This represents a three-fold [XXX] improvement
over the previous best upper limit.  As we discuss in more depth in the companion
to this paper \citet{pober_et_al2015}, this upper limit supports and extends previous
evidence for significant heating in the intergalactic medium prior to reionization.  Under the assumption
that heating comes from X-rays, this result indicates an X-ray background that exceeds
what would be expected to be produced from the currently observed quasar population at high redshifts.
This probably indicates significant contributions from quasars at lower luminosities that have not
yet been observed. [XXX cross-check all of this, see if there are any stronger statements].  We
conclude with a discussion of implications for future 21cm reionization experiments, including
the newly funded Hydrogen Epoch of Reionization Array (HERA).
\end{abstract}

% XXX fringe weighting profile XXX delay spectrum not violated by freq-dependent
% fringe rate weights

\section{Introduction}

The {\it cosmic dawn} of the universe, which begins with the birth of the first stars and ends approximately
one billion years later with the full
reionization of the intergalactic medium (IGM), represents one of the last 
unexplored phases in cosmic history. 
Studying the formation of the first galaxies and their influence on the primordial IGM during this
period is among the highest priorities in modern astronomy.
During our cosmic dawn, IGM characteristics depend on the cosmic density field, the mass and clustering of 
the first galaxies \citep{lidz_et_al2008} [XXX check], their ultraviolet luminosities [XXX],
the abundance of X-ray sources and other sources of heating \citep{mesinger_et_al2013} [XXX check],
and higher-order cosmological effects like the relative velocities of baryons and dark matter \citep{visbal_et_al2012}.

Recent measurements
have pinned down the bright end of the galaxy luminosity function
at $z \la 8$ \citep{bouwens_et_al2010,schenker_et_al2013} and have detected a few sources at even greater
distances \citep{ellis_et_al2013,oesch_et_al2013}. Still, this known population falls well short 
of the requirements to reionize the universe \citep{robertson_et_al2013} [XXX is this the right cite?], 
driving us to deeper observations with, e.g., JWST and ALMA, to reveal the fainter end of the luminosity function.
In parallel, a number of indirect techniques have constrained the evolution of the neutral fraction
with redshift. Examples include integral constraints on reionization from the
optical depth of Thomson scattering to the CMB \citep{planck_et_al2014},
large-scale CMB polarization anisotropies \citep{page_et_al2007}, and
secondary temperature fluctuations generated by the kinetic Sunyaev-Zel'dovich effect \citep{zahn_et_al2012,mesinger_et_al2012}.
Other examples probing the tail end of reionization include
observations of resonant scattering of Ly$\alpha$ by the neutral IGM toward
distant quasars (the `Gunn-Peterson' effect) \citep{fan_et_al2006},
the demographics of Ly$\alpha$ emitting galaxies \citep{schenker_et_al2013,treu_et_al2013,Faisst_et_al2014},
and the
Ly$\alpha$ absorption profile toward very distant quasars \citep{bolton_et_al2011}.

%In addition to using the 21 cm transtion as a probe of the early universe, many
%other techniques have been used to constrain reionization parameters such as
%the redshift of reionization (when the neutral fraction, $\bar{x} = 0.5$), the
%duration of reionization, and the end of reionization, amogst others. The
%Gunn-Peterson effect has shown that the intergalactic medium was fully ionized
%by $z=6$ (\cite{fan_et_al2006}). The most recent results from SPT have shown
%the constraints of the duration of reionization to be  $\Delta{z} \textless 5.4$
%(\cite{george_et_al2014}) using results from the kinetic Sunyaev-Zel'dovich
%effect. The optical depth parameter, $\tau$, from the CMB also constrains
%reionization with the Panck results (\cite{planck_et_al2014}) and CMB
%polarization (\cite{page_et_al2007}). Lyman emitters and lyman alpha absorption
%profiles towards distant quasars have also given limits on the end of
%reionization
%(\cite{schenker_et_al2013,treu_et_al2013,faisst_et_al2014,bolton_et_al2011}.
% XXX merge with above


Complementing these probes of our cosmic dawn are experiments targeting
the 21~cm ``spin-flip" transition of neutral hydrogen at high redshifts.
This signal has been recognized as a potentially powerful probe
of the cosmic dawn \citep{pritchard_loeb2012,morales_wyithe2010,furlanetto_et_al2006} that can reveal
large-scale fluctuations in the ionization state and temperature of the IGM, opening
a unique window into the complex astrophysical interplay between the first luminous
structures and their surroundings.
Cosmological redshifting maps 
each observed frequency with a particular emission time (or distance), enabling 21~cm experiments 
to some day reconstruct 
three-dimensional pictures of the time-evolution of large scale structure in the universe. 
While such maps can potentially probe nearly the entire observable universe \citep{mao_et_al2008},
in the near term, 21~cm cosmology experiments are focusing on statistical or integrated measures
of the signal.

%The hyperfine transition in the ground state of hydrogen
%leads to the emission of the infamous [XXX: infamous?!] 21 cm wavelength photon. The 21 cm
%transition is thought to be one the most promising probes of the early universe
%due to the fact that the frequency of observed radiation maps to a specific
%redshift. It has the potential to map the never before seen dark ages
%($z\sim{30}$), where the first galaxies and stars were beginning to form, as
%well as when the Epoch of Reionization (EoR), when neutral hydrogen was
%completely ionized by the photons emitted by the first luminous galaxies. The wealth of
%information that can be obtained from the use of this optically thin transition
%include the formation of the first stars and galaxies, neutrino masses, and
%initial inflation power spectrum, amongst other science (XXX: list and cite
%exhaustively). Using 21 cm to map the universe has the ability to surpass the
%Cosmic Microwave Background (CMB) in terms of the wealth of science it can
%deliver (see, e.g., \cite{furlanetto_et_al2006}, \cite{morales_wyithe2010}, and
%\cite{pritchard_loeb2012} for reviews on using the 21 cm transition as a
%cosmological probe).
%XXX Should ideally separate out cosmological applications from astrophysical ones.

There are two complementary experimental approaches to accessing 21~cm emission from
our cosmic dawn.  So-called ``global" experiments such as 
EDGES \citep{bowman_et_al2010}, 
the LWA \citep{ellingson_et_al2013},
LEDA \citep{greenhill_bernardi2012}), 
DARE \citep{burns_et_al2012}, 
SciHi \cite{tabitha_et_al2014}, 
BigHorns \citep{sokolowski_et_al2015},
and SARAS \citep{patra_et_al2015} 
seek to measure the
mean brightness temperature of 21 cm relative to the CMB background. These experiments
typically rely on auto-correlations from a small number of dipole elements to access
the sky-averaged 21~cm signal, although recent work indicates
that interferometric cross-correlations may also be used to access the signal
\citep{presley_et_al2015,vedantham_et_al2015}.
In contrast, experiments targeting statistical power-spectral measurements of the 21~cm
signal employ larger interferometers.  Examples of such interferometers targeting
the reionization signal include
the GMRT \citep{paciga_et_al2013},
LOFAR \citep{van_haarlem_et_al2013},
the MWA \citep{tingay_et_al2013},
the 21CMA \citep{peterson_et_al2004,wu2009},
and the Donald C. Backer Precision Array to Probe the Epoch of Reionization (PAPER;\citealt{parsons_et_al2010}). 

Relative to its competitors, PAPER is unique for being a dedicated instrument with the flexibility
to explore nontraditional experimental approaches.  PAPER has converged on a unique, self-consistent
approach to achieving both the level of foreground removal and the sensitivity that are required in principle
to detect the 21cm reionization signal.  This approach focuses on spectral smoothness as the primary
descriminant between foreground emission and the 21cm reionization signal  and applies an understanding
of interferometric responses in the delay domain to identify bounds on instrumental chromaticity 
\citep{parsons_et_al2012b}.  This type of ``delay-spectrum'' analysis permits data from each 
interferometric baseline
to be analyzed separately without requiring synthesis imaging for foreground removal.  As a result, PAPER has
been able to adopt new antenna configurations that are densely packed and highly redundant.
These configurations are poorly suited for synthesis imaging but
deliver a substantial sensitivity boost for power-spectral measurements that are not limited by
cosmic variance \citep{parsons_et_al2012a}.  Moreover, these array configurations are particularly suited
for redundancy-based calibration \citep{liu_et_al2010,wieringa1992,zheng_et_al2014}, on which PAPER
now relies to solve for the majority of the internal instrumental degrees of freedom.
It remains to be seen how PAPER's experimental approach will bear out in
practice, although recent results with the 32-antenna version of PAPER, including the previous best upper 
limit on the 21 cm power spectrum at $\Delta^2 (k) \leq (41\,\textrm{mK})^{2}$ at 
$k=0.27 h \text{Mpc}^{-1}$ \citep{parsons_et_al2014}, indicate the efficacy of the approach so far.


In this paper, we report on the results from a 64-element deployment of PAPER that substantially
improve on this previous result.
 This paper is outlined as follows. In section
\ref{sec:observations} we describe the observations used in this analysis. In
\ref{sec:improvements} we discuss the improvements in this pipeline with respect
to the previous analysis of PSA-32 \cite{parsons_et_al2014}. We then move on to
the data analysis pipeline in section \ref{sec:analysis}. Seciton
\ref{sec:results} describes the results of our efforts and provides new
contraints on EoR. We conclude in \ref{sec:conclusion}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Observations}\label{sec:observations}

\begin{figure}[!t]\centering
\includegraphics[width=\columnwidth]{plots/antenna_positions.png}
\caption{
[XXX need bigger text in figure]
Antenna layout for the PAPER 64 observation season. We use a subset of
the total number of baselines for in the power spectrum analysis. We use only
the East-West single column separation baselines (49-41,41-47,47-19,10-3,3-25,
etc...), the near East-West baselines with a North-South component equaling
$+1$ row spacing (10-41,9-3,3-47,58-25, etc...) and the $-1$ spacing
[XXX don't like ``+1" and ``-1" notation for paper]
(49-3,10-58,41-25,3-1, etc...). These baselines constitute the bulk of the
sensitivity of for the power spectrum analysis.}
\label{fig:antenna_positions}
\end{figure}


[XXX all citations need to be citep, citet, or citealt]

The results in this paper are based on drift-scan observations 
with 64 dual-polarization PAPER antennas deployed 
at the Square Kilometer Array South Africa
(SKA-SA) reserve in the Karoo desert in South Africa
(30:43:17.5$^\circ$ S, 21:25:41.8$^\circ$ E).
Each PAPER element features a crossed-dipole design measuring two
linear (X,Y) polarizations that are orthogonal along boresight but deviate
from perfect orthogonality off axis.  The design of the PAPER element, 
which features spectrally and spatially smooth responses 
down to the horizon with a $60^{\circ}$ FWHM is summarized in \citet{parsons_et_al2010}
and \citet{pober_et_al2012}.  
[XXX make sure \cite{jacobs_et_al2011} and \cite{stefan_et_al2013} are cited somewhere].
For this analysis, we use only the XX and YY polarization cross-products.

As shown in Figure \ref{fig:antenna_positions}, PAPER-64 employs
a highly redundant antenna layout where multiple baselines measure
the same Fourier mode on the sky
\citep{parsons_et_al2012a,parsons_et_al2014}.
We rely on all 2016 baselines for each (XX,YY) polarization cross-product for calibration,
but only use a subset of the baselines for the power spectrum
analysis. This subset encompasses 3 separation types; the shortest 
east-west baselines (e.g. 41-47 in Figure \ref{fig:antenna_positions}) and those of the same east-west column spacing,
except slanted up and down one row (e.g. 10-41 and 10-58, respectively).
[XXX tighten language about "slanted"]
Baselines in each of the 3 separation types (or redundant groups) are
instantaneously redundant within the group and therefore measure the same
Fourier modes on the sky. This allows us to coherently add Fourier modes,
building sensitivity as $N$ rather than $\sqrt{N}$, where $N$ is the number of identical baselines.
[XXX make an effort to clearly define a "separation" and use that language throughout paper].

Observations with this 64-antenna dataset spanned a 135 day period 
from 2012 November 8 (JD62456240) to 2013 March 23 (JD62456375). 
For the power spectrum analysis, we use observations between 0:00 and 10:00 hours
local sidereal time (LST). This range corresponds to
a ``cold patch", in which galactic synchrotron power is minimal (away from
the center of the galaxy). Due to LST drift of 3:56 minutes every solar day, the interferometer
unevenly samples LST's in our observation, giving us more samples of certain
lsts than others. (XXX: Strange to have this be in the active voice, since 
it's not like you're choosing to get different LST sampling on purpose)
The lsts with the greatest number of samples are those used in
the power spectrum analysis (along with those corresponding to the cold patch).
Figure \ref{fig:coverage} shows our observing field with the contours labeling
the beam weighted observing time relative to the peak, directly over head the
array.
%XXX lst range will change.

The PAPER-64 correlator processes a 100 MHz to 200 MHz bandwidth, first
channelizing the 100 MHz band into 1024 channels of width 97.6 kHz, and then
cross multiplying every antenna and polarization with one another for a total of
8256 cross products, including auto correlations. The architecture of this
correlator is based on CASPER open source, reconfigurable hardware
(\cite{parsons_et_al2008}). The channelizer, which includes the PFB-FFT and
corner turn, consisted of 16 ROACH boards with eight 8 bit analog to digital
converters. The cross multiplication engine (X engine) was moved to GPUs based
on the X-GPU, a general purpose X engine for GPU's \citep{clark_et_al2013}. 
Visibility cross products are integrated for 10.7 seconds on the GPU's before
being written to disk. We write all stokes parameters to disk; however the
analysis described here only apply to the XX and YY polariztion products.


\begin{figure*}[!t]\centering
\includegraphics[width=2\columnwidth,height=\columnwidth]{plots/coverage.png}
\caption{The Global Sky Model (GSM) with the PAPER field overlayed during this
observing season. The contours shown here are the beam weighted obsering times
relative to the peak observing time.}
\label{fig:coverage}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Calibration and Analysis}\label{sec:analysis}

Foreground contamination and signal sensitivity represent the two major concerns for 21~cm
experiments targeting power-spectral measurements. Sources of foregrounds include
galactic synchrotron radiation, supernova remnants, and extragalactic radio sources.
In the low-frequency radio band (50--200 MHz) where 21~cm reionization experiments operate,
emission from these foregrounds
drowns out the faint reionization signal by more than 5 orders of magnitude
\citep{deoliveira2008,jelic_et_al2008,santos_et_al2005}.
However, the brightest foregrounds are spectrally smooth, and this provides an important hook
for their isolation and removal \citep{liu_tegmark2012,petrovic_oh2011,liu_et_al2009}.  Unfortunately, interferometers, which are inherently chromatic
instruments, interact with spectrally smooth foregrounds to produce unsmooth features that
imitate line-of-sight Fourier modes over cosmological volumes \citep{parsons_et_al2012b,bowman_et_al2009,morales_et_al2006a}.
One approach to solving this problem involves an ambitious calibration and modeling approach to spatially localize and
remove foreground contaminants \citep{chapman_et_al2013,sullivan_et_al2012,harker_et_al2009,liu_et_al2008,bowman_et_al2008}.
Perhaps the most impressive example of this approach is being undertaken by LOFAR, where dynamic ranges of 4.7 have
been achieved in synthesis images \citep{yatawatta_et_al2013}, although it is expected that additional
suppression of smooth-spectrum foreground emission will be necessary \citep{chapman_et_al2013}.

The analysis for this paper employs a contrasting
approach based on the fact that the chromaticity of an interferometer
is fundamentally related to the length of an interferometric baseline.  This relationship, known
colloquially as ``the wedge", was 
derived analytically \citep{parsons_et_al2012b,vedantham_et_al2012}, and has been confirmed in 
simulations \citep{datta_et_al2010,hazelton_et_al2013} and observationally
\citep{pober_et_al2013,dillon_et_al2013b}.  As described in \citep{parsons_et_al2012b}, the wedge is the result of the delay
between when a wavefront originating from foreground emission
arrives at the two antennas in a baseline.  The fact that this delay is bounded by the light-crossing
time between two antennas (which we call the ``horizon limit'' since such a wavefront would have to have
been generated from foregrounds sitting on the horizon) places a fundamental bound on the chromaticity of
an interferometric baseline.  So far, PAPER has had the most success in exploiting this bound
\citep{parsons_et_al2014,jacobs_et_al2014}, although it is in the process of being applied to MWA
observations \citep{nitya}.  

XXX blend transition to actual analysis steps


\begin{figure*}
%\includegraphics[]{}
\caption{
Flow of data through the power-spectrum analysis steps described in \S\ref{sec:analysis}.
[XXX finish this].
}
\label{fig:flowchart}
\end{figure*}

We now describe the analysis pipeline leading up to the final power spectrum
measurement of our data set. To begin, we run a compression algorithm to reduce
the data volume of our raw data. This step, described in the appendix of
\cite{parsons_et_al2014}, reduces data volume by a factor of 40. We then
calibrate on the basis of redundancy before using self-calibration
to solve for the absolute phase and gain parameters. After suppressing
foregrounds with a wide-band delay filter, we average the data in local
sidereal time and apply a fringe rate filter to optimally combine the
time domain data. Finally, we use optimal quadratic estimators to make our
estimate of the 21 cm power spectrum.  Figure \ref{fig:analysis_pipeline} shows
a block diagram of the analysis pipeline and includes some information on
simuations and signal loss.

\subsection{Calibration}\label{sec:calibration}
The calibration procedure employed in this analysis is twofold: a relative
calibration and an absolution calibration. The relative calibration employed
uses the fact that PAPER contains numerous redundant baselines. As usual, the
absolute calibration uses the usual self calibration methods employed by radio
interferometers. Here we describe both procedures.

\subsubsection{Relative Calibration}
%    -Principles of redundant calibration. Fact that there is no signal loss in
%       this kind of measurement.
%       --PAPER has a redundant configuration. Therefore the number of baselines
%         is larger than the number of unique baselines in the array. There are
%         multiple baselines that measure the same sky.
%       --Introduce uniqe baseline. We define a unique baseline to be the set of
%         redundant baselines of a unique orientation.
%       --All baselines of a unique baseline measure the same sky and
%         therefore, differences between them are what need to be calibrated
%         out. These are the gain variations imparted on the incoming signal by
%         the antenna.
%       --There is no signal loss in this method. Sky signal is the same for
%         each of the redundant baselines is the same and hence any algorithm that
%         preserves common mode signal (which is what redcal does) is necessarily
%         lossless. This is an important point!  
%       --In addtion, because gains are normalized to have a unity magnitude the
%         input arn output flux scale are the same. (This is an omnical thing)
%         
%    -What can we solve for and what can't we solve for. We can solve for the
%     relative complex gains between antennas, but not the absolute gain and phase.
%       --Because redundant calibration does not fold in any outside
%         information about the sky that we are observing, it inherently is a
%         relative calibration scheme that solves for the relative gains and
%         phases of antennas within the array. 
%       --Therefore, we cannot solve for the absolute phase and gain of the
%         array. There is not enough information.
%       --Absoulte phase and gain calibration are addressed in later sections.
%

The highly redundant configuration of PAPER lends itself to
use the redunancy in baselines for a relative calibration between anteannas.
Multiple baselines of the same length and orientation measure the same sky
signal. On this basis, all of the differences between redundant baselines are
attributed to differences in the signal chain, including effects from variations
in antennas, cables lengths, and receivers. These are the variations that are
calibrated out and leads to relative calibration solutions between all the
antennas within the array.  Note that this relative calibration using redundancy
(i.e. redundat calibration) only constrains the relative complex gain solutions
between antennas and has no information about the sky. Furthermore, redundant
calibration inherently preserves common mode signal between redundant baselines,
and therefore this type of calibration is necessarily lossless. 


%    -Summary of the procedure of redundant calibration. This should be mixed in
%     with the equations from Zheng et. al.
%       --Two flavors of redundant calibration : logcal and lincal. Cite
%         liu,zheng.
%       --logcal takes log of equation 3 in zheng et. al., and thus becomes a
%         linear system. Can then solve for the log of the antenna gains, as
%         well as the true visibility of the sky as measured from a unique
%         baseline. 
%       --lincal is the linearization via taylor expansion of the same equation
%         3. This provides us with a better solution. The reason for doing
%         lincal is that logcal is a biased estimator. 
%       --Because there are more baselines than the number of unique baselines, 
%         we have an overdetermined system of equations and therefore can
%         uniquely solve for all of the gains and "true" visibilities.
Practiaclly, redundant calibration takes on two flavors, log calibration (logcal) and
linear calibration (lincal) (\cite{liu_et_al2010,zheng_et_al2014}). Logcal, takes
the logarithm of the visibility measured by an interferometer (equation 4 of
\cite{zheng_et_al2014}) to give a linearized system which can then be fit to
using linear least square. We are fitting to
\begin{equation}\label{eqn:logcal}
    \log{v_{ij}} = \log{g_{i}^{*}} + \log{g_{j}} + \log{y_{i-j}},
\end{equation}
where the $g_{i}$'s are the
complex gains of each antenna and $y_{i-j}$ is the visibility of the given
baseline if it measured the perfect sky. Redundancy gives us an overconstrained
system of equations and we can thus solve for complex gains from each antenna
using traditional linear algebra techniques.  In addition we also get a model
visibility of for a given given baseline.  Averaging these models together for
redundant baselines gives us a an estimate of our model visibility for a given
baseline type.

Lincal taylor expands the visibility around some initial estimates of the gains
and model visibilities via the equation 
\begin{equation}\label{eqn:lincal}
v_{ij} = g_{i}^{0*}g_{j}^{0}y_{i-j}^{0} + g_{i}^{1*}g_{j}^{0}y_{i-j}^{0} +
         g_{i}^{0*}g_{j}^{1}y_{i-j}^{0}+g_{i}^{0*}g_{j}^{0}y_{i-j}^{1},
\end{equation}
where the $0$ indexes are the intial estimates and the $1$ indexes are the
solutions we fit for. In the implementation, described below, the initial
estimates of the solutions are taken as the outputs of logcal. The benefit of
lincal is that it is not a biased estimator as opposed to logcal which takes the
logarithm of additive noise(\cite{liu_et_al2010}).

%    -How was this calibration applied to the data. 
%       --Using omnical package. Give credit jeff and url to omnical. Cite
%         paper.
%       --Implements both logcal and lincal. Discuss the speed ups. 
%       --gains were applied to the uv datasets and written out in the same
%         format. However, Omnical is quite general and solutions can be written
%         out in text files and adapted to other file formats.
%    
%    -Removing additive offset and what is the time cadence of all of this.
%       
%    -Diagnostic figures : Chi-squared, complex plane, stability vs. time and
%                          frequency.

Implementaion of redundant calibration was done with the ${Omnical}$ package, an
open source redundant calibration package written by Jeff Zheng
\footnote{https://github.com/jeffzhen/omnical}\cite{zheng_et_al2014}. This
package implements both logcal and lincal, solving for a complex gain solution
per antenna, frequency, and integration. The per integration solutions are then
applied to visibilities and the results are shown in \ref{fig:omniview}.

\begin{figure*}
\centering
\includegraphics[width=2\columnwidth]{plots/omniview_64.png}
\caption{Real-Imaginary plot of the complex visibilities measured by all of the
baselines with a cursory calibration (left) and after applying solutions
obtained with Omnical (rigth). Each shape and color combination correspond to
a set of redundant baselines. }
\label{fig:omniview}
\end{figure*}

\begin{figure}[!b]
\centering
\includegraphics[width=\columnwidth]{plots/august_chi2.png}
\caption{chi squared. This plot should be horizontal. ask jeff.}
\label{fig:chi2}
\end{figure}


In addition to solving for gain solutions, Omnical also characterizes the
quality of the calibration by calculating the $\chi^{2}$ for every integration.
Figure \ref{fig:chi2} shows the $\chi^{2}$ squared waterfall for a full
nights worth of calibrations. From \cite{zheng_et_al2014}, the
$\chi^{2}/\text{DoF}$, where $\text{DoF} = 2N_{baselines} - 2(N_{antennas} +
N_{\text{unique basleines}} $ is the number of degrees of freedom. For a good
fit, the $\chi^{2}/DoF$ is drawn from a mean 1 distribution with variance
$\frac{2}{\text{DoF}}$. The distribtuion of the $\xi^{2}$ in figure
\ref{fig:chi2} has a mean of 2, meaning that the fit was not the best to what
our noise model is.
The noise model used was obtained from the variance statistic calculated during
the binning of days in local sidereal time, which occurs after the
application of crosstalk removal, combining X and Y polarization, and the
wideband delay filter. Hence, the noise model may not be well suited for the
$\chi^{2}$, and is a likely reason that the distribution has a mean of 2.
%XXX This may change and we may get a chi-squared of around one.% 

The calibration parameters solved for in Omnical also have a time independent
additive offset associated with them, This additive offset is associated with
crosstalk between antennas, especially the shorter north south baselines.
To remove this crosstalk, Omnical calculates an averaged (over ten minutes)
additive offset that it subtracts from each visibility before running fitting
for solutions once again. This additive offset is in effect averaging deviations
from the $chi^{2}$ over 10 minutes and subtracting that deviation off.

\subsubsection{Absolute Calibration} 
%    -Selfcal to pictor, fornax, and crab (to get the north-south components).
%       --Reiterate that redundant cal does not solve for the overall flux and
%         phase parameters. 
%       --There are two final phase parameters we must solve
%         for in order to correctly phase to a source on the sky. Redundancy
%         only gets you so far, but still need to be able to unambiguously phase
%         to sources.
%       --We use self calibration to solve for the global phase parameters using
%         Pictor A, Fornax A, and Crab Nebula. 
%       --
%
%    -Diagnostic Plots: Show Field image from Bernardi. This will give
%     confidence in a good absolute phase calibration.

After solving for the relative complex gains of the antennas using redcal, an
overall phase and gain calibration needs to be solved for. We use the standard self
calibration (selfcal) method for radio interferometers to solve for the absolute
phase calibration. We used Pictor A, Fornax A, and Crab Nebula in our selfcal
fit for the overall phase solutions. Figure \ref{fig:field_image} shows an image
of the field with Pictor A (5:19:49.70, -45:46:45.0)  and Fornax A
(3:22:41.70,-37:12:30.0). The measured source positions are the same as the the
catalogue positions for these sources.



%    -Calibrated to Pictor A. 
%       --As before, redundant calibration cannot set a global flux scale. For
%       our measurements to be correct, we need to set a fluxscale. We use
%       Pictor A to set our fluxscale. Cite Dannys pictor paper.
%      
%    -Our bandpass model is a 9th degree polynomial. 
%       --In order to set our fluxscale, we beamform our data upto pictor A,
%         summing baselines, and then then fit a 9th degree polynomial to the
%         bandpass. This is our measure of the spectrum of Pictor A. 
%       --Because we are beamforming to pictor and fitting a polynomial to the
%         bandpass, there is signal loss. This signal loss is of the 
%    -Tabulate signal loss due to this model. Averaging over Nbls, times gives us
%     an average over independent uv modes.
%       --Working on this... a few questions about it.
%
%    -The actual signal loss on a given mode is L/N where L is the loss for a
%     single instance of the beamform (one time, one baseline) and N is the
%     number of baseline and times that were summed.
%
%    -PLOTS: 
%        --A plot that is the phased to pictor beamform gain 
%          (or maybe just a single channel for all days.
%        --The measured and theoretical pictor spectrum 
%        --A comparison to the PSA32 Flux CAL.

We then set our over all flux scale by using Pictor A as our calibrator source
with source spectra as derived in \cite{jacobs_et_al2013}, 
\begin{equation}
    S_{\nu} = S_{150}\times\left(\frac{\nu}{150MHz}\right)^{\alpha},
\end{equation}
where $S_{150} = 382 \text{Jy} \pm 5.4$ and $\alpha = -.76 \pm .01$, with
1$\sigma$ error bars.


%To derive the source spectrum from our measurements, we use lst averaged data (see
%section \ref{sec:lstbin}) containing foregrounds for the hour before and after from
%where Pictor A transits. We image, in 15 minute snapshots for this data set,
%every frequency channel. The source spectra is derived per snapshot and we average
%these together, weighting by the primary beam in the direction of Pic A to get
%the measured spectrum of Pictor A. To fit our bandpass, we divide the model
%spectrum with ourAmeasured spectrum and fit a ninth-order polynomial over a
%frequency range from 120-170MHz. Figure \ref{fig:pic_spec} shows the derived
%Pictor A spectrum and the model spectrum derived from \cite{jacobs_et_al2013}.


To derive the source spectrum from our measurements, we use LST-averaged data
(see section \ref{sec:lstbin}) prior the wide-band delay filter, for the hour
before and after the transit of Pictor A. We image a $30^\circ \times 30^\circ$
field of view for every frequency channel for each 10 minute snapshot and apply
uniform weights to the gridded visibilities. We account for the required three
dimensional Fourier transform in wide field imaging by using the w-stacking
algorithm implemented in WSclean \citep{offringa_et_al2014} â€“ although we note that
the standard w-projection algorithm implemented in
CASA\footnote{http://casa.nrao.edu} gives similar performances as the PAPER
array is essentially instantaneously coplanar.  A source spectra is derived for
each snapshot by fitting a two dimensional Gaussian to Pictor A by using the
PyBDSM\footnote{http://www.lofar.org/wiki/doku.php?id=public:user\_software:pybdsm}
source extractor. Spectra are optimally averaged together by weighting them with
the primary beam model evaluated in the direction of Pictor A. To fit our
bandpass, we divide the model spectrum by the measured one and fit a 9-th order
polynomial over the 120-170 MHz frequency range. Figure \ref{fig:pic_spec} shows
the calibrated Pictor A spectrum and the model spectrum from
\cite{jacobs_et_al2013}.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{plots/gianni_pic_image.pdf}
\caption{Image of Pictor.}
\label{fig:field_image}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{plots/PicA_normalized_spectrum.pdf}
\caption{Pictor spectrum.}
\label{fig:pic_spec}
\end{figure}

Fitting a polynomial to the bandpass has the potential for signal loss which
would include fitting out some modes of the EoR fluctuations. In order to
quantify the signal loss associated with fitting a ninth degree polynomial to
the bandpass, we run a Monte Carlo simulation of the effect the bandpass has on
an EoR signal. We construct a bandpass resulting from the fitted
polynomial and add a low level EoR signal on top of that bandpass, modeled as a
white noise with a unformly varying phase, for each baseline and independent
time mode due for the beamforming to Pictor A. The total number of independent
modes we average over before fitting a 9th degree polynomial to the simulated
signal is 1017, which inlcued number of unique baselines and the number of
independent modes over the 2 hours we used to measure the spectrum source. We
finally compare the output amplitude of the EoR signal to the input amplitude in
power spectrum domain and find that between $-.06 < k < .06$, the signal loss is
less than $3\%$ and at the mode right outside the horizon is $.0000002\%$. The
signal loss descreases for higher $k$'s.

%These lists were My bullet points.
%\begin{itemize}
%    \item{Overview of the calibration with emphasis on Omnical.}
%    \item{Rough Calibration : Can cite Parsons 2014 for the details.}
%    \begin{itemize}
%        \item{Discuss the first pass of redundant phase and gain calibration.
%              Absolute phase calibration using pictor, fornax and crab in the
%              fit.}
%        \item{Absolute flux scale using Pictor A. Want to address signal loss.
%             Maybe delay the issue of signal loss to a "Signal Loss" section
%             which takes into account the signal loss in various stages of the 
%             analysis?}
%        \item{Add in details of the loss of eor signal. Do simulations to get
%              numbers. }
%        \item{Plots : Measured pictor spectrum with comparison to the Danny
%              pictor spectrum. Maybe some simulation plots showing the signal
%              loss due to the polynomial fit to pic spec.}
%    \end{itemize}
%    \item{Omnical}
%    \begin{itemize}
%        \item{What is omnical?}
%        \item{Why are we using it? To get a frequency/time dependent
%calibration. Need to address why this is not overfitting. That is why is this
%just not flatting everything out.}
%        \item{Address non-possibility of signal loss.}
%        \item{Refresh formalism (do this here or in the intro of calibration?)}
%        \item{What is it doing? logcal/lincal. Chi-squared and how that
%              determines the goodness of the fits to the model baselines. What
%model are we using? Is it a good model?}
%        \item{Plots: Chi-squared plots. Compex plane plots that show
%              improvements over uncalibrated/rough/omnical calibrated. 
%              Time and frequency stability.}
%    phase to pic gain, pic spec, comparison to psa32.
%    \end{itemize} 
%\end{itemize}

%OLD TEXT START
%\subsubsection{Overview}
%As the name suggests, redundant calibration (\cite{liu_et_al2010},
%\cite{zheng_et_al2014}) uses redundancy within the array to solve for relative
%phases and gains of the antennas. To explain redundant calibration, suppose that
%the baseline between antennas $i,j$ measure a visibility $v_{ij}$, then we have 
%
%\begin{equation}\label{eqn:redcal}
%    v_{ij} = g_{i}g_{j}^{*}y_{i-j} + n_{ij},   
%\end{equation}
%where $g_{i}$,$g_{j}$ are the complex gains due to antenna $i$ and antenna $j$,
%respectively, $y_{i-j}$ is the true visibility measured by perfect antennas
%$i$,$j$ for the given baseline type, and $n_{ij}$ is the residual noise from
%the baseline.  If the number of baselines of a given type is much greater than
%the number of baselines types this problem is over constrained and $g_{i}i$,
%$g_{j}$, and $y_{i-j}$ can be solved for. PAPER is in this limit due to the
%maximally redundant configuration as shown in figure \ref{fig:antenna_pos}. 
%
%Redundant calibration comes in two flavors: log calibration and linear
%calibration. Log calibration, or logcal for short, takes the logarithm of
%equation \ref{eqn:redcal} to give a linearized system. Hence, solutions can be
%solved for by using standard linear algebra techniques. However, this method is
%biased. On the other hand linear calibration, or lincal for short, is an
%unbiased method of solving for the complex gain solutions. In this method
%equation \ref{eqn:redcal} is Taylor expanded about an initial guess for the
%$g_{i}$'s and $y_{i-j}$'s to give a linearized equation which can be used to
%solve for the complex gains and sky model. 
%
%In this analysis we used a logcal algorithm based in delay space to get a rough
%calibration of the dataset. This was followed by an absolute calibration to set
%the overall phase and flux scale using a self calibration. We used model phase
%centers of Pictor, Fornax A, and Crab Nebula. The absolute amplitude is set
%by the flux of Pictor A found in \cite{jacobs_et_al2013}, whose spectrum is
%defined by 
%\begin{equation}
%    S_{\nu} = 382(\frac{\nu}{150 MHz})^{-.76} Jy.
%\end{equation}
%
%Finally, we used the Omnical calibration package to do another round of
%redundant calibration to get even more accurate calibration parameters.
%
%\subsection{logcal-for lack of a better title}
%We first perform the same calibration that was
%done in \citep{parsons_et_al2014a}. That is, we use redundancy to do a relative
%phase\footnote{In actuality, we solve for delays to get around phase wrapping
%issues. These delays are applied to visibilities as $e^{2\pi{i}\tau\nu}$}
%calibration between antennas, which removes the electrical delays from cables in
%the signal path. Due to redundancy, we can calibrate out all of the per-antennas
%delays in the signal path relative to two delay parameters which we call
%$\tau_{ns}$ and $\tau_{es}$. These delays are the relative electrical delays
%that correspond to baseline delays in the north-south and east-west component
%for 2 reference baselines (49-10 and 49-41,respectively). These solutions were
%then applied to the data set which was calibrated again with Omnical. 
%
%The application of this calibration to the data set before Omnical was needed
%because in order to calibrate accurately, Omnical needs to have a rough estimate
%for the calibration solutions for every antenna. In \cite{zheng_et_al2014}, a
%model of the sky was used in order get the rough estimate of the solutions.
%Here, we use actual sky data to get the rough calibration. Because the solutions
%are derived from the instrument, we can incorporate into the solutions antenna
%based variations. 
% 
%The antenna based
%delay solutions vary as much as a couple nanoseconds day to day when solutions
%are averaged over hour long timescales withing a day. However, the variations in
%solutions is worse when only averaging over ten minute time scales. Therefore
%need for better calibration is requred.  We use self calibration to derive the
%two unknow parameters, $\tau_{ns}$ and $\tau_{ew}$, by using the Crab Nebula,
%Fornax A, and Pictor A.
%
%Note that there is no possibility of signal loss (see \citep{parsons_et_al2014a}).
%
%\subsection{Gain Calibration}
%Gain calibration was derived on the basis of redundancy and self calibration.
%The phase calibrations described above, simultaneously also calibrated for the
%relative gain variation between antennas. Again we can only calibrate to a fiducial
%antenna (49) whose gain is defined as unity. We then perform a self calibration
%to set the flux scale to Pictor A whose spectrum is derived in
%\citep{jacobs_et_al2013}. We use the same methods describes in \citep{parsons_et_al2014a}.
%
%Figure \ref{fig:bmfom_pic} shows that dataset beamformed to Pictor A, with log
%janskies on the y axis and lst on the xaxis for a frequncy of .1 + (120/203)*.1/203. 
%As can be seen, the day to day variation in the formed beam has a fractional
%spread of about 10$\%$.  This shows the stability of the instrument and the well
%behaved calibration solutions derived above. 
%
%\subsection{Omnical}
%(How did we know that our calibrations were not good enough? Because of the power
%spectrum? PSA32? We did beamform data to pictorA and say that vs LST, the
%beamform matched well day to day with a fractional spread of about 10$\%$) 
%
%The complex gain solutions found in the previous calibration pipeline were
%averaged together in time and one solution per frequency was used for the array.
%This jived with the philosophy that the array was stable in time and frequency.
%However, upon further review of this data set, it seemed more and more likely
%that this was not the case anymore. (Is this even true? What specifically? Think
%Man!) 
%
%Due to clues that showed that our data set had time dependent calibration
%solutions, it was imperative that we do a better job at calibrating our array.
%
%The Omnical redundant calibrator
%package\footnote{https://github.com/jeffzhen/omnical} (omnical) performs
%redundant calibration for every time and frequency in a dataset using both
%logcal and lincal methods as described in \cite{zheng_et_al2014}. It also
%contains methods on the quality of the solutions by providin a chi-square for
%the fits to the data. 
%
%For this dataset, omnical first performed a logcal (again) to attain a solution
%per time and frequency. This solution was passed to lincal which iteratively
%solved for the complex gain solutions. The convergence criteria was when the
%$\chi^{2}$ decreased by less than $.01\%$. The $\chi^{2}$ for the fit used in
%Omnical is given by 
%\begin{equation}
%    \chi^{2} = \sum_{ij}|v_{ij} - y_{i-j}g_{i}^{*}g_{j}|^{2},
%\end{equation}
%
%which differs from normal nomenclature because we are not inverse varaince
%weighting. Note that this $\chi^{2}$ is summing over all baselines and hence 
%giving more weight to higher gains. Note that omnical fits for each of the
%complex gains and the model visibility, $y_{i-j}$,  for a unique baseline.
%Using this information, figure \ref{fig:chi_2} shows that the $\chi^{2}$ is close
%to 1 for all channels and time (for this day of data). need noise model for
%this.
%
%Figure \ref{fig:gain_solutions} shows the gain solutions output by omnical. The
%amplitude of the gains are roughly order unity through out. These are relative
%gains between antennas and hence the over flux scale set to Pictor A is still
%valid. The absolute calibration is still valid. 
%
%Since Omnical outputs a model visibility of what a unique baseline should
%measure, which is derived from the data by removing all of the variation between
%unique types of baselines and averaging, we are able to use these outputs as our
%dataset. Infact, this is what is done. 
%%waterfalls of chi squared and solutions.
%%day to day repeatability.
%%The output of the omnical - 
%%
%OLD TEXT END



\subsection{Wideband Delay Filtering}\label{sec:wbd_filtering}
%   -Can cite Parsons 2014a
%       --We use the same wbd filter as in Parsons 2014a. That is we do a per
%         baseline delay filtering with a buffer of 15 nanoseconds. 
%   -Quantify signal loss. 
%       --I think this is the same as before. Since delay filtering is done on a
%       per baseline basis, the signal loss from the previous paper and this one
%       is the same. We are using the same filter.
%   -PLOTS:
%        --waterfalls of before and after cleaning.
%        --signal loss vs. k_parallel


Before implementing our foreground removal techniques, we combine the two
polarizations for a coarse estimate of stokes I as per \cite{moore_et_al2013}.
[ XXX provide the relevant equations and caveats here, careful with "coarse" --- don't imply sloppy]
Namely, the Stokes I estimate is estimated as $V_{\rm I} = \frac12(V_{\rm XX}+V_{\rm YY})$. This estimate
neglects the beam mismatch between the beam of the two polarizations, which
leads to polarization leakage from Stokes Q into Stokes I, contaminating the
measurement. We offer no correction for this, but in turn put some limits on
polarization leakage discussed later.

Amongst the cornucopia of foreground removal techniques (cite papers) at our
disposal, we opt for the per-baseline approach of delay filtering described in
\cite{parsons_et_al2012b}. This technique is well suited for PAPER because it
doesn't rely on making images and localizing sources for subtraction, which 
the current maximum redundant configuration of PAPER is not well suited for.

The delay transform method for localizing sources relies on the fact that
foregrounds are spectrally smooth. For a given visibility, the delay transform
is defined as the Fourier transform of a visibility with respect to frequency, 

\begin{align}\label{eqn:delay_transform}
\tilde{V}(\tau) &= \int{W(\nu)S(\nu)A(\nu)B(\nu)I(\nu)
                   e^{-2\pi{i}\tau_{g}}e^{2\pi{i}\tau\nu}d\nu} \\
                &= \int{W(\nu)S(\nu)A(\nu)B(\nu)I(\nu)
                   e^{-2\pi{i}\nu(\tau_{g}-\tau)}d\nu} \\
                &= \tilde{W}(\tau) \ast\tilde{S}(\tau) \ast \tilde{A}(\tau) \ast
                   \tilde{B}(\tau) \ast \tilde{I}(\tau) \ast
                   \delta(\tau_{g} - \tau),
\end{align}
where $A(\nu)$ is the frequency dependent beam, $B(\nu)$ is the bandpass,
$W(\nu)$ is a Blackman-Harris taper function to minimize band edge effects,
$S(\nu)$ is the frequency sampling function due to RFI flagging  and $I(\nu)$ is
the source spectrum. In delay domain, a smooth spectrum source is a delta
function at delay $\tau_{g}$ (a flat spectrum source's delay spectrum)
convolved by the spectrum shape, the beam, the bandpass shape, the taper
function and the sampling function. The source shape, $A(\nu)$, and $B(\nu)$,
imprint a width onto the delta function and leads to leakage of power outside
of the horizon limits imposed by the baseline length.  As per
\cite{parsons_et_al2010} and \cite{parsons_et_al2014}, we deconvolve the kernel
resulting from $W(\tau)S(\tau)$ using an iterative CLEAN procedure
\citep{hogbom1974} restricting CLEAN components to fall within the horizon plus
a 15 ns buffer for each baseline that accounts for for shape of the source
spectrum and the use of the limited bandwidth used in the delay transform,
which is limited by $1/B \approx 10 ns$. To remove the smooth spectrum
foreground emission we subtract the clean components from the original
visibility.

\begin{figure*}[!t]
\centering
\includegraphics[width=2.3\columnwidth]{plots/waterfalls.png}
\caption{LST vs. frequency waterfall plots of LST averaged data for the entire
observing season with log amplitude on the top and phase on the bottom row.
Foreground averaged data (left) is fringe rate filtered with the optimal fringe
rate filter (second left) and signal is retained, but signal low in the beam is
down weighted. Visibilities have a wideband delay filter applied to them to
suppress foreground signal (middle)i which achieves a 2 orders of magnitude
signal suppression in Jansky's. After applying the optimal fringe rate filter to
the foreground filtered data (second right), we achieve 3 orders of magnitude
suppression of foregrounds, but the fringe rate filter also suppresses the
cosmological signal as well. However, because the optimal fringe rate filter is
an time averaging filter, it has the effect of de-noising the data, suppressing
noise even more than the signal, increasing the SNR. Finally, averaging all of
the redundant 30 m East-West baseline, effecting 4 orders of magnitude below the
foregrounds in Jansky.} 
\label{fig:waterfalls}
\end{figure*}

For the 30 m East-West baselines used in the power spectrum analysis,
foregrounds are suppressed by $\sim$5 orders of magnitude in power, effectively
getting -50 dB of foreground suppression, as seen in figure
\ref{fig:waterfalls}, for any given baseline. There is an associated signal loss
with the delay filter technique due to the deconvolution used in the CLEAN
algortithm. This signal loss is the same as in \cite{parsons_et_al2014} because
we are using same filter with the same 15 ns buffer outside the horizon per
baseline. We reiterate the results here: there is a 4.8\% signal loss for the
first mode outside of the horizon, 1.3\% for the next mode out, and less than
.0015\% for the higher modes.

After the wideband delay filter, we conduct another round of RFI and crosstalk
removal which was overshadowed by the foregroun signal. For RFI excision we
apply a median filter which flags above $3\sigma$. Foregrounds kept us using the
traditional method of removing cross talk which consisted of subtracting hour
long averages from each visibility. This was due to the fact that some days in
the observation had gaps in time owning to some technical difficulties during
observations. However, with foregrounds removed we were able to remove 10 minute
averages from every visibility within a file (files have a cadence of ten
minutes). Normally, hour long averages are needed for foreground contained data
to wash out the fringes from the said foregrounds to detect the static phase
bias that is crosstalk. For foreground removed data, we do not have this
complication since bright foregrounds are not dominating the average and are
able to remove the offset by subtracting shorter sums.

Once smooth sources have been removed and a final pass of RFI excision and
crosstalk removal have been performed, the data is averaged in local sidereal
time (LST) with bins of width 43 seconds to match the fringe rate filter applied
in the compression. The averaged data set consisted of 135 unevenly sampled
days with the effective number of total days being 123.57. 

Sporadic RFI events can skew individual LST bins away from the median value of
the sky at a given LST, deviating from gaussian statistics. To catch these
events, we compute the median of a LST bin for each frequency and flag off 3
sigma outliers of the median before averaging. This filter mitigates
the effects of non gaussianity in time, which is most likely due to spurious
RFI events. The median statistics used here necessitate that there is no signal
loss due to this filter.

We make two separate lst binned data sets, averaging every other julian day
together to obtain an "even" and "odd" dataset. The use of these two data sets
allows us to conststruct an unbiased power spectrum estimate, as well as running
jackknife tests on our observations. 



%\section{LST Binning and Stability}\label{sec:lstbin}
%   -Practicals: bin size, number of days, range of days 
%       --We LST bin the data over the 120 night data set into time bins of
%         42.95 seconds. 
%       --WE form multiple LST data sets which bin different days throughout the
%       observation together. These datasets help us remove systematics in the
%       power spectrum estimation. They also provide us a way to jackknife the
%       data set.
%   -N lst data sets. Jack-knifing etc.
%   -Median Filter : no signal loss. Filtering because of outliers in time that
%    find their way into the data 
%       --While lst binning, we apply a median filter which for a given lst and
%       frequency bin, removes data which falls outside 3 sigma of the median of
%       the dataset. This filtering is necessary, because of the non gaussian
%       events that crept their way into the data set, such as RFI, etc...
%       --There is no signal loss associated with this because we are using
%       median statistics. 
%   -Plots: Integration counts vs. LST/freq  waterfall.






%\subsubsection{A noise study}
%During the LST averaging, we compute the median and the variance for every LST
%and frequency bin. The variance in particular is of importance becuase it allows
%us to estimate the system temperature, $T_{sys}$, as a function of LST and
%frequency. The variance is computed, per frequency, for all the visibilities
%that are included in a given LST bin, which gives us an estimate ${I_{rms}}$,
%the specific intensity in Jy, which is then converted to a $T_{rms}$ in the
%usual way, 
%\begin{equation}
%    T_{rms} = \frac{I_{rms}\lambda^{2}}{2k\Omega}.
%\end{equation}
%
%where $\lambda$ is the observing wavelength, $\Omega$ is the size of the beam in
%steradian, and $k$ is the boltzmann constant. We convert $T_{rms}$ to a system
%temperature by scaling up the rms with the effective integration time and
%bandwidth used. That is, 
%\begin{equation}
%    T_{sys} = T_{rms} \times \sqrt{\Delta{B}t_{int}}.
%\end{equation}
%
%Figure \ref{fig:tsys_lst_fq} shows the system temperature as a function of
%LST and frequncy. In our "cold" patch, we find that $T_{sys}$ is around $500K$.
%
%
%\begin{figure}
%\centering
%\includegraphics[width=\columnwidth]{plots/tsys_lst_freq.png}
%\caption{Tsys as a function of lst and frequency. The cold spot resides in the
%lst range of 1-7hours.}
%\ref{fig:tsys_lst_fq}
%\end{figure}

\subsection{Optimal Fringe-Rate Filter}\label{sec:frf}
%   -What is fringe rate filtering. Cite Parsons And Liu 2014.
%       --Fringe Rate filtering is nothing more then a time domain weighted
%         average per frequency of of visibilities. The filter can be applied 
%         as either a convolution in the time domain or a multiplicative fourier
%         filter in fringe rate space. 
%       --We first calculate fringe rate filter such that we upweight the more
%         sensitive fringe rates versus the not so sensitve fringe rates by
%         weighting the fringe rates for a given baseline by its beam.
%   -Effective beam areas.
%       --When fringe rate filtering, we are upweighting fringe rates that
%         are most sensitive. That is we are weighting these fringe rates by the
%         beam of a baseline. This has the effect of narrowing the effective
%         beam. It turns out that we are removing more noise than signal,
%         because we are essentially keeping the most sensitive parts of the
%         sky. This actually gives us a sensitivity boost of about a factor of
%         2. 
%   -effective integration time and number of modes. 
%       --Do calculations.
%       --Fringe rate filtering is effectively a weighted average in time and
%       thus there is an effective integration time. We can calculate this
%       integration time by noting that power (or variance) is a conserved
%       quantity. Therefore, comparing the integral of a variance=1 noise signal
%       when it is fringe rate filtered to when it is not, gives us a fractional
%       integration time. 
%           \frac{ \int_{t_{start}}^t_{end}{\sigma^{2}*F_{constant}(t)dt
%           }}{\int_{t_{start}}^t_{end}{\sigma^{2}*F_{filter}(t)dt = fraction.
%            t_int = fraction * t_start-t_end
%           Something like that.
%       --Due to the fact that fringe rate filtering is effectively averaging in
%         time, A fringe rate filter reduces the number of independent modes on
%         the sky. The number of modes in an entire day drop just 45
%         (24hrs/1900sec)  independent modes on the sky. 
%   -PLOTS: 
%       --FR filter (both in fr and time space), applied to data.
%       --Beam after fringe-rate filter. 
%       --Waterfalls.
%       -- apply filters to foreground data. Compute fr of pica and show that we
%          are not killing the sky.
%   
%   code to get the integration time of fringe rate filter. This is 1886 for channel 100
%   beam_w_fr = frf_conv.get_beam_w_fr(aa, (1,4))
%   t,firs,frbins,frspace = frf_conv.get_fringe_rate_kernels(beam_w_fr, 42.8, 401)
%   fr100 = frspace[100]
%   t_int = 42.8/n.mean(fr100)
%   

Prior to forming power spectra we need to time average visibilities that measure
the same $k$ mode on the sky. The motivation for this is that we want to combine
coherent $k$-modes on the sky before squaring to get the maximum sensitivity. We
therefore get a $\frac{1}{N}$ sensitivity boost, rather than averaging in $P(k)$
which would only give us a $\sqrt{N}$ sensitivity benefit. However, rather than
a straight average, we can apply a carefully crafted fringe-rate filter that
weights higher sensitivity portions of the sky higher than those lower in the
sky.

Broadly, for a given baseline and frequency, different parts of the sky
correspond to different fringe-rates, albeit not a one-to-one correspondense.
Maximum fringe rates are found along the equatorial plane, where the rotation
rate of the earth is highest, and zero fringe rates are found at the poles,
where the rotation rate of the earth is zero and hence sources do not move
through your baelines fringe pattern. We can use this fact to pick out certain
points on the sky, but because this mapping of fringe-rate to sky is not one to
one, we can only pick out stripes of constant fringe rate.

As motivation, we discuss the use of fringe rate filters in cross talk removal.
Crosstalk is modeled as a time independent coupling of signal between two
different signal paths, be it at the antenna or to to input our the analog to
digital converters. Since crosstalk does not vary as a function of time, in
fringe rate space, cross talk piles up at the poles which correspond to zero
fringe rates. In this view crosstalk is a DC bias. To remove crosstalk, we can
apply a notch filter to the fringe rate transform of the time series visibility
(per frequency) and inverse transform to go back into time domain. This
interpretation of crosstalk being a DC offset jives with our method of crosstalk
removal discussed above. In order to remove a DC offset from a time series, we
subtract the average.

Now, to optimally combine the time data we can cater our fringe rate filter to
upweight points of the sky that contain more signal to our instrument and down
weight those points that do not. That is, if we weight fringe rate
bins by the beam, we can get a net increase in sensitivity. Roughly, all fringe
rate bins contain the same amount of noise in them, but the amount of signal
varies and is determined by how the primary beam illuminates the sky.
Upweighting the bins with higher signal relative to those with less signal
gives us a net increase in signal-to-noise, even though we are removing 
signal by applying signal from this filter. 

The effective beam area from applying an optimal fringe rate filter decreases,
which implies that the signal has been attenuated, as noted above. But because
the noise is attenuated more, because of beam weigting fringe rate bins, the SNR
increases. For the PAPER array, the boost in SNR is a factor of 2. The net boost
in SNR depends on where the array is located. 

We implement the optimal fringe rate filter by calculating the fringe rates at
every point on the sky, for a given frequency, and weighting each bin by the
beam of a given baseline. It is worth noting that fringe-rate filtering is
baseline dependent and hence is implemented on a per baseline basis. We weight
the fringe rate bins on the sky by the beam pattern at the same point on the
sky. In order to obtain a smoothe filter in the fringe rate domain, we fit a
gaussian with a tanh tail to this filter as shown in figure
\ref{fig:fringe_rate_cut}. We then fourier transform this to obtain the time
domain filter, multiply by a blackman-harris window function to damp the tails
of the filter, and finally convolve with the visibilites to effect time domain
integration.

Since the PAPER beam is ~45 degrees, and the array is located at a declination
of $-30^{\circ}$ the fringe rates associated with the low signal to noise (down
in the beam) correspond to very high and very low/negative fringe rates. Figure
\ref{fig:fringe_rate_cut} shows a cut of the optimal fringe rate at 159 MHz for
a 30 m east west baseline. Therefore, the implemented fringe rate filter removes
some sky signal, signal associated with fringe rates outside of the ranges shown
in Figure \ref{fig:fringe_rate_cut}. Figure \ref{fig:fr_preserved_signal} shows
that the applied filter removes sky associated with negative fringe rates and
very high fringe rates. 

%\begin{figure}[!t]
%\centering
%\includegraphics[width=\columnwidth]{plots/fr_filter_slice.png}
%\caption{
%slice of a fringe rate filter at a frequency of 159MHz. Top is the
%filter in fringe rate domain. The bottom consists of the corresponding time
%domain filter gotten by fourier transforming and windowing with a
%blackman-harris window to damp the tails.
%[XXX move this figure into fringe-rate filter paper]
%}
%\label{fig:fringe_rate_cut}
%\end{figure}

\begin{figure*}[t!]\centering
\includegraphics[width=2\columnwidth]{plots/fr_preserved_signal.png}
\caption{
[XXX combine 3 panels into 1, simplify real/imag/abs if necessary].
Slice of a fringe rate filter at a frequency of 159MHz. Shown here is
the fringe-rate transform of foreground contained data for a 30 m east-west
baseline. Blue and green are the real and imaginary part, respectively and red
is the absolute valeu. Note the maximum and minimum fringerates correspond to
the theoretical minimum and maximum for a baseline of this length at 159 MHz.
The center panel shows the real (red)  and imaginary (green) parts of the fringe
rate filter to be applied. Finally, the last panel on the right shows that the
fringe rate filtered visibilities. The fringe rate filter is just a weighting
applied in fringe rate space and retains foregrounds.
[XXX clearer description of what fringe rate means]
}
\label{fig:fr_preserved_signal}
\end{figure*}

For the power spectrum baselines in question (30 m East-West baselines), the
effective integration time is calculated by comparing the variance statistic for
a fringe rate filter that is a flat weighting vs that with an optimal filter
weighting as described. We apply these filters to random noise with variance 1,
without loss of generality. We can conclde that 
\begin{align}
    t_{int} &= \frac{\int{\sigma^{2}dt}}{\int{\sigma^{2}fr^{2}dt}}\\
            &= 1886 \text{seconds},
\end{align}
for an optimal fringe rate filter on a 30 m East-West baseline at 149.5 MHz.
Over the range of frequencies used in our power spectrum estimate, the
integration times range from 1936 s to 1790 s, correspinding to an average of 31
minutes. Because the fringe rate filter in effect integrates in time, the number
of statistically independent samples of the sky drastically decreases. We are
left with $\sim2$ independent samples per hour.

%\begin{figure}[h!]\centering
%\includegraphics[width=\columnwidth, height=.8\columnwidth]{plots/noise_t_35.png}
%\caption{Estimates of noise temperature. Magenta is frequncy differenced
%estimate where the cyan is the time differenced estimate. All curves are
%averaged over all 30m east-west baselines (56) and averaged incoherently in 43s
%bins of LST from LST 3 to 5 hours with a channel bandwidth of 490 kHz.}
%\label{fig:noise_t}
%\end{figure}


%\begin{figure}[h!]\centering
%\includegraphics[width=\columnwidth, height=.8\columnwidth]{plots/noise_vs_fq_plot.png}
%\caption{Estimates of noise temperature. Magenta is frequncy differenced
%estimate where the cyan is the time differenced estimate. Averaged in LST from 3
%to 5 hours. on uv files calibrated with omnical. fg,delay filtered, baseline
%averaged, normal fringe rate filter, optimal fringe rate filtering.}
%\label{fig:noise_omni_uv}
%\end{figure}


\subsection{Optimal Quadratic Estimator and Covariance}\label{sec:oqe}
%   -Motivate method with discussion of empirical estimate of covariances and
%    problem of limited modes.
%       --Because we dont know the true covariances between baselines and
%         channels we have to estimate the covariances epirically from data. 
%       --We do this by taking the outer product of our data vector (consisting
%         of a visibilies for every baseline of a given unique baseline type).
%         The outer product is calculated per time sample, thus we average over
%         a "clean" (what do i mean by this?) lst range.
%       --Hence, the quality of the estimate of the covariances between
%         baselines depends on the average in time ( the ensemble average). 
%       --The fact that we do not know what the full covariance matrix looks
%         like leads to signal loss. We have an estimate of the covariance, not
%         the true covariance matrix. There is no way to truly know what the
%         covariance matrix is for our data set. 
%       --After fringerate filtering, the number of independent modes on the
%         decreases substantially because we are averaging over 1900 seconds
%         ~=31.67 minutes. Therefore the number of independent modes decreases
%         1900/(the beam in time). 
%       --In our range of lsts used, we only have roughly 20 independent
%         samples. Therefore we end up with an ill determined covariance matrix
%         to describe our data. 
%       --In addition, because of the highly redundant nature of the array,
%         the covariance matrix is highly singular and therefore not invertible.
In this section We first
review the quadratic estimator formalism, with an emphasis on our power
spectrum analysis, followed by a walk through of our application of the OQE
method to our data, and finally discuss the effects of an empirical estimate of
covariance matrices.



%
%   -Mathematics compared to ideal optimal Quadratic Estimator.
%       --In the quadratic estimator formalism, the value of the power spectrum,
%         $p_{\alpha}$  in the $\alpha^{th}$ bin is given by
%         $Mq_{\alpha}=$Mx^{\dagger}C{-1}Q_{\alpha}C^{-1}x, where x is a vector
%         containing the binned data in frequency domain, $x = V(\nu)$, C is the
%         estimate of the covariance matrix of of our data, C = <xx^{\dagger}>,
%         $Q_{\alpha}$ is defined such that $Q_{\alpha}=\frac{dC}{d\alpha}$, and
%         M is a normalization matrix that normalizes the estimator.
%       --Generally, the covaraince matrix used is the full covaraince between
%         all baselines and channels. 
%       --Because of redundancy, the matrix is singular and not invertible. We
%         therefore construct a pseudo inverse for C. In addtion to the psaudo
%         inverse we take only the auto-baseline covarainces, masking out the
%         covariances between baselines. This, is invertible. WE can thus apply
%         the inverse of C in the above equation.
%
%   -Counting of independent modes.
%       --Number of independent modes = 2*#of lst hours used in analysis. This
%         is because 1900 seconds ~ 30 minutes.
\subsubsection{Review}
We use the optimal quadratic estimator method to estimate our power spectrum as
done in \cite{dillon_et_al2013a} and \cite{liu_tegmark2011}.  Here we briefly review the
optimal quadratic esitimator (OQE) formalism with an emphasis on our application
to data. The end goal of this analaysis technique is to estimate the 21 cm
power spectrum, $P_{21}(\k)$, defined such that 
\begin{equation}
\label{eqn:pspec_def}
    \expval{\tilde{T}_{b}(\k)\tilde{T}^{*}_{b}(\k^{\prime})} =
            (2\pi)^{3}\delta^{D}(\k - \k^{\prime})P_{21}(\k),
\end{equation}
where $\tilde{T}_{b}(\k)$ is the fourier transform of the brightness temperature
distribution on the sky, $\expval{}$ denote the ensemble average, and
$\delta^{D}$ is the Dirac-Delta function. 

In order to make an estimate of the power spectrum, we first begin with a data
vector $\x$ which includes all of the measured  visibilities (both in time and
frequency), $V(\nu,t)$, one wishes to use in the estimate of $P_{21}$. We form
the intermediate quantity,
\begin{equation}
\label{eqn:qalpha}
    q_{\alpha} = \frac{1}{2}\x^{t}\mathbf{C}^{-1}\mathbf{Q}_{\alpha}\mathbf{C}^{-1}\x - b_{\alpha},
\end{equation}
which will be needed to form the optimal quadratic estimator of our power spectrum.
Here, $\mathbf{C}$ is the true covariance matrix of the data vector $\x$, 
$\mathbf{Q}_{\alpha}$ is the operator that takes visibilities into power spectrum
$k$-space and bins into the ${\alpha}^{th}$ bin, and $b_{\alpha}$ is the bias to
the estimate that needs to be subtracted off. This bias is the instrumental
noise bias due to the instrument itself. There is infact another source of bias
due to the foregrounds. In general, $\mathbf{Q}_{\alpha}$
is a family of matrices, one for each $\alpha^{th}$ bin (equivalent to $k$-bin
for a power spectrum analysis). It is important to note that
$\mathbf{Q}_{\alpha}$ can take on any form and be completely arbitrary. It is
the data analysts choice. However, if one is looking for the best possible
estimate of the power spectrum,  $\mathbf{Q_{\alpha}}$ generally
takes a form that is proportionl to  $\mathbf{C}_{,\alpha} \equiv
\frac{\partial{\mathbf{C}}}{\partial p_{\alpha}}$, which is the derivative of
the the covariance matrix with respect to the band power $p_{\alpha}$, defined
below.  Therefore, $\mathbf{Q}_{\alpha}$ encodes the response of the data
covariance matrix to the $\alpha^{th}$ bandpower. We describe our choice of this
matrix in the next section, where we describe the application of the quadratic
estimator. 

Normalizing the $\q_{\alpha}'s$ with a matrix $\mathbf{M}$, we form
the final estimate of our power spectrum,
\begin{equation}\label{eqn:pspec_norm}
    \mathbf{\hat{p}} = \mathbf{M}\mathbf{q},
\end{equation}
where $\mathbf{\hat{p}}$ is the optimal quadratic estimate of the power spectrum
for every band power $p_{\alpha}$ in $k$-space, and $\mathbf{q}$ is a vector of the
quantities in equation \ref{eqn:qalpha} for every $\alpha'th$ bin.

The bias term $b_{\alpha}$ in equation \ref{eqn:qalpha} is the the instrumental
noise bias that needs to be subtracted off. However, in our analysis we are
constructing an unbiased estimator by using cross correlations between different
redundant baselines. This eliminates the need to remove the noise bias. On the
other hand the foreground bias is also somewhat accounted for by using cross
correlations. We try to remove this bias by cross correlating between different
days and baselines. Due to the fact that we are using cross correlations in our
quadratic estimator of the power spectrum, equation \ref{eqn:qalpha} becomes, 
\begin{equation}\label{eqn:qalpha_unbiased}
    q_{\alpha} =
\frac{1}{2}\x_{1}^{t}\mathbf{C}^{-1}\mathbf{Q}_{\alpha}\mathbf{C}^{-1}\x_{2} - b_{\alpha},
\end{equation}
where $x_{1}$ and $x_{2}$ are visibility vecotrs that correspond to two
different redundant baselines. This cross correlation is the key to obtaining a
zero bias estimate of the power spectrum. The cross correlation is a crucial
difference in the quadratic estimator method used by others [XXX is this
true?].

The normalization matrix above is defined such that
\begin{equation}\label{eqn:window_def}
    \mathbf{W} = \mathbf{M}\mathbf{F}, 
\end{equation}
where $\mathbf{F}$ is the Fisher information matrix, given by
$\mathbf{F}_{\alpha\beta} =
\frac{1}{2}tr(\mathbf{C}^{-1}\mathbf{Q}_{\alpha}\mathbf{C}^{-1}\mathbf{Q}_{\beta})$
and $\mathbf{W}$ is the window function matrix. The window functions measure the
degree to which power from $k$ bins couple into the measurement of the power
measured in the $\alpha$'th $k$ bin. Specifically, 
\begin{equation}\label{eqn:true_pspec_2_est_pspec}
    \hat{\mathbf{p}} = \mathbf{W}\mathbf{p}, 
\end{equation}
where $\mathbf{p}$ and $\hat{\mathbf{p}}$ are vectors of the true power spectrum and our
estimate of the power spectrum for every $\alpha$'th bin, respectively. Note
that the window functions are normalized such that the rows of $\mathbf{W}$ sum
to unity.

%   -Cholesky Decomposition and window functions.
%       --The optimal window functions (that minimize the vertical error bars of
%       the estimate, are given by the inverse of the Fisher matrix. However,
%       there is a trade off such that this incorporates information from all of
%       the k-modes. 
%       --Some other choices of the window functions are the square root of the
%       fisher matrix or the identity. 
%       --We use the cholesky decomposition of the Fisher Matrix such that we
%       can write F = LL^{\dagger}, where L is a lower triangular matrix. This
%       is possible for any hermitian positive-definite matrix.
%       --We use L for our window functions because for every k-bin, it does not
%       use information from the k-bins below it. And thus there is no mixing of
%       modes lower than it. 
%       --This is particularly important because it doesn't insures that there
%       is no leakage of the modes within the horizon to modes outside of it.         
%
The normalization matrix is the analysts choice, and is the last chance to
weight the measurements in the estimates of the power spectrum; it is
essentially a rebinning of the data. One choice that can be made is to pick
$\mathbf{M} = \mathbf{F}^{-1}$ implying that $\mathbf{W} = \mathbf{I}$. This
window function is a delta function window such that every band power does not
contain leakage from other band powers. Therefore, each band power estimate
contains power from only that bin. However, this has the down side that the
error bars are large and hence the estimate is not very well localized. On the
other hand, choosing the identity normalization such that the window functions
are given by the Fisher matrix, leads a minimum error on the error bars, but
with the consequence that every band power is coupled to every other band power.
Th choice of $\mathbf{M} = \mathbf{D}$, where $\mathbf{D}$ is a diagonal matrix,
produces the smallest possible error bars [XXX cite result]. $\mathbf{D}$ is
diagonal and not the identity because the window functions are normalized such
that the rows of the matrix sum to unity.  

In addition to calculating the window functions, we can also calculate the error
correlations that correspond to a given window fuction. In order to see the
effect of the normaliztion matrix on the error correlations we start by looking
at the covariance of the band power estimates, namely, 
\begin{equation}\label{eqn:err_cov}
    \boldsymbol \Sigma = \text{Cov}(\hat{\mathbf{p}}) = \expval{\hat{\mathbf{p}}\hat{\mathbf{p}^{\dagger}}} -
             \expval{\hat{\mathbf{p}}}\expval{\hat{\mathbf{p}}}^{\dagger}.
\end{equation}
Since $\phat = \mathbf{M}\qhat$, it is easily shown that 
\begin{equation}
    \Sigma = \mathbf{M}\mathbf{F}\mathbf{M}^{\dagger},
\end{equation}
where $\mathbf{F}$ is the Fisher matrix. The error correlation matrix describes
how correlated the errors between modes are in the final power spectrum result.


\begin{figure}[t!]\centering
\includegraphics[width=\columnwidth, height=\columnwidth]{plots/window.png}
\caption{Window function matrix. The i'th row corresponds to the window function
used in the estimate of the power spectrum for the i'th $k$-mode.}
\label{fig:window_func}
\end{figure}


\subsubsection{Application}
% A walk through of the application to the data.

Here we describe the specifics of our application of the optimal quadrtic
estimatore formalism to measure the power spectrum. First, we describe our data
set used in the application. Our data set consists of visibilities as a function
of frequency and time for each baseline in the array. In estimating the power
spectrum we use specific redundant baseline, applying the analysis to
different redundant baselines separtately. In the description what follows we
use the the east-west 30 meter redundant baselines ( baselines $49\_41$,
$41\_47$, $10\_3$, etc. in figure \ref{fig:antenna_positions}), but we apply the
same anaysis to the 30 m baselines with one row spaceings (10-41,9-3,3-47,...
and 49-3,10-58,41-25,... antennas in figure \ref{antenna_positions}), combining
the final power spectra in the bootstrapping after estimating power spectra.
Focusing on the 30 m east-west redundant baselines, our data vector becomes a
list of visibilities such that, 

\begin{equation}
\label{eqn:xvectdef}
\mathbf{x} = \left( \begin{array}{c}
V (t_{1}) \\
V (t_{2}) \\
\vdots \\
\end{array}
\right), 
\end{equation}
where, $V(t_{i})$ is the visibility of a baseline, indexed by LST bin,
belonging to the redundant baselines under consideration. Each visibility
contains 10 MHz of bandwidth centered at 151.5 MHz, amounting to 20 channels. 
Notice that we have only included data from one baseline, which is looking ahead
to the application of the method our data. We in effect apply the quadratic
estimator on a per baseline basis and sum the baselines together to get our
final estimate. The important point is that the data vector is in frequency
space, where our final power spectrum estimate is in fourier space. 

This leads to the description of the matrix that takes the data vector,
$\mathbf{x}$, and transforms it into something that is in the power spectrum
domain, $\mathbf{Q}_{\alpha}$. Because our data matrix is in frequency domain,
we need a way of tranforming that data into $k$-space, and specifically into
the line of sight $k_{\parallel}$-modes. As shown in \cite{parsons_et_al2012b},
line of sight $k$-modes are approximated by delay modes for short baselines and
therefore the $\mathbf{Q}_{\alpha}$ should encode the delay transform. This is
the infact the form of this matrix we use; we encode the delay transform in to
the $\mathbf{Q}_{\alpha}$ by taking the outer product of a sine wave with a
given delay mode, $\alpha$,  with itself.  Therefore, we construct a family of
$\mathbf{Q}$ matrices, each with a specific delay transform measuring a single
delay mode. [XXX include plots of application of Q to data?]

According to equation \ref{eqn:qalpha}, there is one more fundamental piece of
information we require, the covariance matrix, $\mathbf{C}$. The covariance
matrix of the data vector $\x$, by definition, is the ensemble average of
$\x\x^{t}$, namely $\mathbf{C} = \expval{\x\x^{t}}$. However, due to our limited
universe, we empirically estimate the covariance matrix by taking the time average of the
quantity $\x\x^{t}$ over a range of LST's. We discuss the implications of
emprically estimating the covarinace matrix in a later section, due to the fact
that the cusp of the problems with signal loss are attributed to this estimate.

The covariance matrix plays an important role in the optimal quadratic estimate
of power spectra. For ease of notation, let us define 
\begin{equation}\label{eqn:z}
    \mathbf{z} =  \mathbf{C}^{-1}\mathbf{x},
\end{equation}
which is the weighting of the data by the inverse covariance. This is a crucial
step in the analysis since it supresses covariant signal between $k$-modes by
down weighting that signal. The application of the covariance matrix to data has
the same general suppression, however, different data sets have different
covariances. Figure  \ref{fig:inv_cov} shows what happens when one applies the
inverse covariance to a data vector that contains foregrounds, and therefore
highly covariant,  and one which has the foregrounds removed via that wideband
delay filter described in section \ref{sec:wbd_filtering}.  In the figure, the
top row corresponds to the the data vector $\mathbf{x}$, which is a waterfall of
the visibilities with frequency on the $x$-axis and time on the $y$-axis. The
middle section shows the empirical estimate of the covaraince by taking the
outer product of $\x$ with itself and collapsing over the time axis. Finally,
the last row is the application of inverse covariance weighting to the data,
namely $\mathbf{z}$.  Applying the invsere covariance to $\x$ down weights the
strongest eigen modes of the covaraince matrix, $\mathbf{C}$, and upweights the
weakest eigen modes. In comparison, for random white noise, the optimal
weighting scheme is inverse variance weighting, which leads to the smallest
error bars. For an infinite sample of random white noise, the covariance matrix
would be diagoanl and applying the inverse covariance would be identical to
inverse variance weighting. The diagonal of this inverse covariance matrix are
the eigenvalues and correspond to the the eigenmodes. Therefore, the most
downweighted modes correspond to the largest eigenvalues of $\C$, and hence the
strongest modes.  Similarily, applying the inverse covariance to the baseline
data vector, $\x$, the strongest modes get down weighted. 

Looking at $\mathbf{z}$ in figure \ref{fig:inv_cov} for the visibilities that
still contain foreground signal (left half) shows the down weighting of
the strongest modes which correspond to foregrounds. The strongest modes here,
are the strongest eigen modes of the the covariance matrix. Figure
\ref{fig:eigs} shows the eigen spectrum and the four strongest eigen modes of a
covaraiance matrix derived from a data vector with foregrounds. As seen four
strongest eigen modes are highly suggestive of smooth spectrum sources. In
contrast, the right half of figure \ref{fig:inv_cov} shows the application of
inverse covariance weighting to data which has the wideband delay filter applied
to remove smooth spectrum foregrounds. Applying the covariance, is upweighting
the weakest modes in the data and since the strongest, foreground, modes have
been removed we are seeing the upweighting of the noise. This is paralleled by
looking at the eigenspectrum and strongest eigenmodes of the foreground-less
covariance matrix in which the strongest modes do not have any coherent
structure to them and can be considered noise like.


\begin{figure*}[h!]\centering
\includegraphics[width=2\columnwidth, height=5.5in]{plots/inv_cov.png}
\caption{Application of inverse covariance weighting to three different
baselines for foreground contained (left 3 columns) and foreground removed
data(right three columns). Each baselines waterfall (top row) covers a LST range
from 0-8:20 hours and frequencies 146-156 MHz. The covariance matrix (middle
row) are constructed by taking the inner product of the data vector and itself,
summing over the LST axis. The final application of the inverse covaraince to
the data vector, $\x$, is shown in the last row, constituting one half of the
quadratic estimator in the formalism shown here. Weighting the data by the
inverse covariance down weights the strongest modes of the covariance matrix in
the data, nominally upweighting the noise in the data as seen in the comparison
of column 4 to column 5, where column 5 is siligtlhy less noisy than the others.} 
\label{fig:inv_cov}
\end{figure*}

\begin{figure*}[t!]\centering
\includegraphics[width=1.5\columnwidth]{plots/eig.png}
\caption{Eigenvalues of the covariance matrix from foreground contained data
(blue) and foreground removed data (green), via the wideband delay filter.
Correspondingly, the strongest four eigenmodes for foreground contained (top
right), and foreground remved data (bottom right).}
\label{fig:eigs}
\end{figure*}

As noted in the previous section, we use cross correlations between baselines
and times to construct an unbiased estimator of the power spectrum. This results
in applying equation \ref{eqn:qalpha_unbiased}, without the bias term, namely, 
\begin{equation}
\label{eqn:qalpha_nobias}
    q_{\alpha} =
        \frac{1}{2}\x_{1}^{t}\mathbf{C}^{-1}\mathbf{Q}_{\alpha}\mathbf{C}^{-1}\x_{2}.
\end{equation}
Here, $\x_{1}$ and $\x_{2}$ are data vectors which have uncorrelated noise
properties leading to the removal of the bias term. As mentioned in section
\ref{sec:wbd}, we have constructed two LST binned data sets, which are identical
in sky signal, but have varying noise properties, owning to being created with
every other day of observation.  This allows us to construct unbiased (in
instrumental noise) quadratic estimator of the power spectrum, but will allow
for a foreground bias if residual foregrounds are still in the data at some
significant level after the application of the delay filter. Concretely, to
remove the bias in the power spectrum we take 2 approaches that should remove
any systematics associated with the power spectrum.


Firstly, we divide the baselines into five different groups without repeating
baselines. For each bootstrap, as discussed below, we randomly sample with
replacement baselines from each group. When computing the estimate of the power
spectrum, we only cross multiply the baselines between groups and never comput
the auto power, minimizing the bias.

Secondly, we use the data from the two LST binned data sets which were created
by binning every other days into the same LST grid. We refer to these data sets
as the "even" and "odd" data sets. When constructing our estimate of the power
spectrum, we use baselines from these two data sets on either side of the
$\mathbf{Q}$ matrix, being careful not to include the same baseline from the
same data set on both sides. This is another axis on which we are trying the
remove the noise bias.


Taking the above two caveats in to consideration, we then construct our optimal
estimator by applying equation \ref{eqn:qalpha_nobias} to the baseline data. In
order to obtain the final estimate of the power spectrum, we average all of the
baselines together as a function of time, measuring one power spectrum per LST
bin. Therefore, because we are using only the auto covariances to weight the
data vecotrs, that is we are not taking into account the baseline-baseline
covariances when weighting the data, only the frequency frequency covariances
between channels. To understand the application of this quadratic estimator
foramlism to the data, let us begin by defining 
\begin{equation}\label{eqn:presum_oqe}
    \mathbf{z}^{r}_{A} = \sum_{i\in{A}} \mathbf{C}_{i}^{-1}\mathbf{x}_{i},
\end{equation}
where $r$ denotes either \emph{even} or \emph{odd} depending on the LST data set
used and $A$ denotes the group of baselines under consideration. Notice that the
sum occurs over the baselines within a group for a given LST dataset. Using this
definition, our final estimate of the intermediate quantity, $\mathbf{q}$, is
given by 
\begin{equation}\label{eqn:presum_qalpha}
    \qhat_{\alpha} = \sum_{\substack{A,B,r,s\\r\ne{s},A\ne{B}}}\mathbf{z}^{r\dagger}_{A}\mathbf{Q}_{\alpha}\mathbf{z}^{s}_{B},
\end{equation}
where the sum occurs over $A,B,r,s$ where $r\ne{s}$ and $A\ne{B}$. Furthermore,
since our $\mathbf{Q}_{\alpha}$ simply encodes a delay transform, it becomes
separable such that, $\Q_{\alpha} =
\mathbf{m}_{\alpha}^{\dagger}\mathbf{m}_{\alpha}$, where $\mathbf{m}_{\alpha}$ a
complex sine wave of a specific frequency corresponding to delay mode $\alpha$.
This is a consequence of approximating delay modes as fourier line of sight
modes, which is a valid approximation for the short baselines under
consideration. If this were not the case, $\Q$, would not be as simple as this
and would be required to take in to account the coupling between delay modes. 
Under this approximation, equation \ref{eqn:presum_qalpha} becomes
\begin{equation}
    \qhat_{\alpha} =
\sum_{\substack{A,B,r,s\\r\ne{s},A\ne{B}}}\mathbf{z}^{r\dagger}_{A}\mathbf{m}_{\alpha}^{\dagger}\mathbf{m}_{\alpha}\mathbf{z}^{r}_{A}.
\end{equation}

After forming intermediate quanitities, we must normalize our power spectrum
estimate to obtain sensible estimates on the power spectrum. As noted above, the
normalization occurs by the $\mathbf{M}$ matrix in equation
\ref{eqn:pspec_norm}, and can be any matrix of our desire. 
Even though the choices of the normaliztion matrices described above have some
good properties, e.g. small error bars and no leakage, we opt for a different
choice of window function as follows. We first take the Cholesky-decompositon of
the Fisher matrix such that $\mathbf{F} = \mathbf{L}\mathbf{L}^{\dagger}$, where
$\mathbf{L}$ is a lower triangular matrix\footnote{This is possible for any
positive definite matrix}. Then, we choose ${\mathbf{M}} = \mathbf{L}^{-1}$ to
be our normalization matrix. In this case, the window function matrix becomes,
$\mathbf{W}=\mathbf{L}^{\dagger}$. Note that we first reorder the rows of the
Fisher matrix before decomposing into a product of upper and lower triangular
matrices. The reordering scheme puts higher $k$-modes first and lower ones last
in the list. After decomposing, this choice of $\mathbf{M}$ matrix has the
effect of only coupling modes exterior the $\alpha$'th mode only to the band
power in $\alpha$  for the final estimate of the power spectrum.  With the
ordering described above, this has the consequence of not coupling the modes
inside the horizon in to measurments of band powers outside the horizon. Figure
\ref{fig:window_func} shows these window functions. 

In addition to the nice properties in the estimates ofthe power spectrum given
by the above normalization and window function matrix, there is another nice
property about the choice of normalization which corresponds to the error
correlation. Using the choice of normalization matrix described above, we have
that \begin{equation} 
    \Sigma = \mathbf{L}^{-1}\mathbf{L}\mathbf{L}^{\dagger}\mathbf{L}^{-\dagger}
           = \mathbf{I}.
\end{equation}
Therefore, for $\mathbf{M}=\mathbf{L}^{-1}$, the error correlations are
proportional to the identity matrix. Proportional to the identity matrix because
our choice of $\mathbf{M} \propto \mathbf{L}^{-1}$ due to the normalization
criteria. Each row of $\mathbf{M}$ are normalized to sum to 1 and therefore,
the error correlation, or covariance of the estimate of our band powers, is only
proportional to the identity matrix, and hence diagonal. This is a very well
behaved property that implies that the error bars between band powers are not
correlated and hence the error bar on every measurement is independent from on
one another. [XXX contrast to the blackmann-harris window function in which
band powers nd errors were correlated with one another. maybe?].

The Fisher matrix is a key component in the above analaysis and contains a
wealth of information about the amount of information in each $k$-mode. The
Fisher matrix is shown in figure \ref{fig:fisher}, represents the amount of
information there is in every $k_{\parallel}$ mode. The Fisher matrix is derived
using all of the baselines, not just a baseline pair and consists of all of the
information contained in the estimate.
%XXX This part needs work. My understanding of the Fisher Matrix is still
%undeveloped. Noisier modes are upweighted in k-space? 

\begin{figure}[b!]\centering
\includegraphics[width=\columnwidth, height=\columnwidth]{plots/fisher.png}
\caption{The Fisher information matrix for a given baseline.}
\label{fig:fisher}
\end{figure}


\begin{figure*}[t!]\centering
\includegraphics[width=1.8\columnwidth]{plots/pspec_variance.png}
\caption{$k$-modes integrating down.}
\label{fig:pspec_variance}
\end{figure*}


\subsection{The Covariance Matrix}
%Covariance matrix nuances.
%   -describe covariance matrix and data vectors.
%   -How we get the covariance matrix. 
%   -eigenvalue decomposition.
%mode counting and nonsingular-ness.
%   -Count independent modes.
%   -covaraiance is independent only if we have enough indendent modes.
%   -inverse covariance is tricky because of this. 
%   -happy coincidence?
%
%XXX need to check lst range. 

We now discuss the issues of empirically estimating the covariance matrix from
the data. For a single baseline, the data vector $\x$, consists of 10 Mhz
bandwidth centered at 151.72 MHz, equating to 20 frequency channels.
All time samples are also kept, ranging from 0 to 10 LST hours. Again, we
estimate the covariance in the usual way, $\C = \expval{\x\x^{\dagger}}$, taking
the ensemble average over time. The covariance matrix for this baseline is a
$N_{ch}\times N_{ch}$ matrix consisting of the covariances of all channels with
each other. This covariance matrix is invertible only if it nonsingular, which
requires it to be a rank $N_{ch}$ matrix. This occurs only if there are $N_{ch}$
independent modes. 

As noted in section \ref{sec:frf} the optimal fringe rate filter corrsponds to
averageing time samples for 31 minutes. Over the 10 hour LST range used in this
analysis, this corresponds to approximately 20 statistically independent modes
on the sky. This is a fundamental limit of the amount of information we have in
our measurements. Coincidentally, the covariance matrix for a given baseline is
an $N_{ch} \times N_{ch}$ matrix, where $N_{ch}=20$.  Therefore, the covariance
matrices for every baseline are infact invertible because they are full rank 
and contain 20 statistically independent measurements of the sky. The covariance
matrix is thus invertible and can be applied to the data vectors, $\x$. 

%
%   -Potential for signal loss.
%       --In a traditional OQE method, the process is lossless by construction.
%         Because we are tweaking this method, and the fact that we have non
%         gaussian systematics in our data, there is a potential for signal
%         loss. 
Even though we have encountered a fortuitous event in having full rank
covariance matrices, we are still estimating our covariance matrix using a
limited sample size. This limitation leads to an imperfect characterization of
the covariances between modes and ultimately leads to signal loss in this type
of analysis. 

The optimal quadratic estimator has an application of the inverse covariance
matrix to data vector $\x$. Applying an inverse covariance matrix to the data it
is derived from has the effect of weighting the data by the covariance matrix.
Therefore strong modes, corresponding to the eigenvectors of the covariance
matrix, are downweighted in this scheme as seen in figure \ref{fig:inv_cov}.
For foreground contained data, the strong modes correspond to the smooth
spectrum foregrounds, and therefore the covaraince matrix is highly covriance
since all of the frequency channels are correlated with one another. However,
for foreground filtered data, the strongest modes correspond to modes that
resemble residual foreground in the data. It is important to note that inverse
covariance weighting is a powerful tool as seen by looking at the three
baselines used in figure \ref{fig:inv_cov}. For the baseline data vector with
the least amount of noise in the delay filtered plot (right half, middle
column), we can see that weighing by the inverse covariance upweights modes in
this baseline as compared to the other baselines, which have a slightly higher
noise in the original data vector. This is even paralleled in the foreground
contained data.


%   -Simulations
%       --In order to show the degree that there could be signal loss, we run
%         simulations to show that an EOR -like signal does pop come out in our
%         data. 
%       --We inject a common eor like signal, a complex random white noise
%         signal, on to all of the baseline data used in this analysisi for a
%         given time. This signal is fringe rate filtered to match the data and
%         therefore, the number of independent modes matches the input data. 
%       --We run this simulation for varying signal levels, showing that this
%         signal can infact be recovered, with some signal loss.
%       --These simulations show that there is signal loss in this method.
%         However, if the signal was bright enough, we would be able to see it.
%         Since we don't see signal in the actual data, we can say that we have
%         not detected the EOR signal yet.
\subsubsection{Signal Loss}
The optimal quadratic estimators for the power spectrum described above are a
lossless operation, in theory. This is only true if we have all the information
needed to apply this method. However, one of the major unknowns we have in our
estimate, is the covariance. We are empirically estimating our covariance matrix
from the data and therefore are potentially going to have signal loss in the
final estimate of our power spectrum. Here we quantify the amount of signal loss
associated with the empirical estimates of the covariance matrix and claim that
it is a monotonic function of input signal level.

In order to understand the degree of signal loss due to our empirical estimates
of the covariances, we simulate the injection of an EoR model and measure the
signal loss associated with running the full covariance analysis. These
simulations consist of injecting a model eor signal on top of the data and
running through our power spectrum estimator. In order to quantify this, vary
the input model EoR signal level that gets added onto the data and run through
the optimal quadratic estimator to measure the signal loss. We compare this to
the same power spectrum pipeline, except that we forgoe the inverse covariance
weighting for no weighting. This is our fiducial power spectrum, which contains
no signal loss. We Monte Carlo to obtain an overall conservative correction
factor for the final power spectrum. 

Before presenting the results we first discuss our model EoR visibilities. For
each measurment set, we construct a random gaussian signal that corresponds 

By using these simulations, we vary the amplitude of the eor signal input into
the oqe by multiplying by a scalar to each simulated visibility. Figure
\ref{fig:simulate} shows that the signal loss is monatonic with increasing
input signal amplitude. With this we argue that if there was an eor signal at
any substatial level, a detection would have been made. Because our power
spectra are still consistent with zero, we conclude that a positive detection of
EoR has not been made. 



XXX Need to claim that we ran an end to end test.

XXX need to claim that because the signal is so low, there is no signal loss.
[ARP: this is not a correct statement.  There is signal loss.  We must quantify it.]

%XXX Carina's Text of the eor model goes here.
The model eor signal used was a simulation of a flat power spectrum, in P($k$),
from $k$-modes ranging from .1-10 $\text{hMpc}^{-1}$. From this an angular power
spectrum is computed (\cite{datta_et_al2007, lewis_challinor_2007}), ensuring
correlation in frequency/redshift for the power spectrum maps. Visibilities are
then simulated for this power spectrum map by explicitly integrating fringes on
the sky every 42.8 seconds for an East-West baselin of 30 m.

[XXX figure about signal loss vs. signal level]
%In order to effectively characterize signal loss in our analysis pipeline, we
%simulate visibilities that accurately capture the instrumental effects of PAPER
%for the frequency bins used in the analysis. The signal injected into the
%simulation is comprised of two components - an artificial power spectrum P(k)
%and frequency extrapolated galactic foregrounds from the Global Sky Model (GSM)
%(de Oliveira-Costa et al. 2008). The injected power spectrum is flat in P(k) for
%a k-range of 0.1-10 $\text{hMpc}^{-1}$, and the angular power spectrum is computed from
%this (\cite{datta_et_al2007, lewis_challinor_2007}), ensuring correlation in
%frequency/redshift for the power spectrum maps. PAPER visibilities are simulated
%for both the GSM and power spectrum maps by explicitly integrating fringes on
%the sky every 42.8s for an East-West baseline of 30m.


%Need to argue that the detections are not gaussian and therefore are not eor
%detections.


%   -Maybe : Problem of low noise measurements in this method of analysis i.e.
%    singular matrics - For Adrian.
%
%   -Boot strapping 
%       --In order to calculate the residual noise in our power spectrum
%         estimate, we bootstrap the baseline samples at the output of the
%         quadratic estimator. 
%       --Doing this gives us the variance of the power spectrum estimate and
%         thus derives the 2 sigma error bars shown in the power spectrum. 
%       --We are careful to not incur a noise bias by making sure that the
%         groups used do not contain the same baselines. However, the pull from
%         each group is random with replacement. Each group can have a repeat of
%         a baseline. 
%       --We use 100 bootstraps to derive our error bars, setting a $2\sigma$
%         bound on the errors.

\subsubsection{Bootstrapped Errors}
%XXX QUESTION FOR BOOTSTRAPPING, Use the input data to the quad est vs output
%data? Is this why the error bars were so wacked out before?
We measure the error in our final power spectrum estimate by bootstrapping over
baseline samples from the output of the quadratic estimator. That is, we are
bootstrapping over power spectra averaged over baselines per LST sample. In order
to tease out the variations between baselines, each bootstrap is a random pull
of the baselines involvled in the analysis, choosing $N-5$ out of $N$
baselines. For different baseline types, $N$ ranges from 46 (up/under one and
over one baselines) to 51 (East-West). 

In order to avoid incurring a noise bias, a few restrictions are needed to the
bootstrapping and power spectrum estimation. First, we break the baselines into
five groups and use the two "even" and "odd" LST averaged data sets discussed
above. We use only baselines in the quadratic estimator that are not the same
as well as data that fall in the different averaged data sets.


\subsection{Integrating $k$-modes}
XXX This shows the efficacy of using the median over the mean.
The power spectrum shown in figure \ref{fig:final_pspec} is an average of
multiple baselines together. To show how the modes in each power spectrum
integrate together and to what degree they integrate down as noise are not, an
allan variance test, is shown in Figure \ref{fig:pspec_variance}. Color
represents a different $k$ mode measured in $\Delta^{2}$ along with the
different bootstraps. Here we are showing the effect of integrating down on
different $k$ modes as we include a wider range in time, and hence more
independent modes. From the discussion on optimal fringe rate filtering we know
we have 2 independent modes on the sky per LST hour. Hence, integrating modes
upto a thousand modes, where not all are independent we expect to hit up against
a floor, which we see. These curves are plotted for $P(k)$ and therefore, the
colors represent both positive and negative $k$'s with green curves haveing the
highest absolute value for $k$ and blue curves are modes closer to the horizon.


We see a slight difference in the mean and median statistic when estimating the
total value of modes. Modes tend to level off for both the statistcs and the
amount of independent modes is capped when modes flatten out at roughly ~60
samples. We find that this is close to the number of independent modes on the
sky and baselines. In addtion modes close to the horizon (blue) tend to flatten
out with the mean statictic, however there is an increased spread for the median
statistic. 

In addtion there is a sharp decline in power when integrating ~1024 modes for
certain modes when dealing with the mean and median statistic. These modes show
unexpected behaviour due to the sharp slope of the fall in power. Further
investigation of these modes is to be conducted.
%


%
%   -PLOTS:
%       --example covariance matrices (with foregrounds, without, fringe rate
%         filtering.
%       --Before and After covariance application waterfall plots.
%
%       --Eigen Spectra and shape of the eigen modes.
%
%       --Window Functions : not waterfalls.
%
%       --Fisher matrices.
%
%       --Power spectrum waterfall plots for different separations. 
%
%       --Maybe the wedge.
%



%\section{Summary of Improvements from PSA32}
%XXX Should this section be here XXX
%In comparison to the previous PAPER pipeline (see \cite{parsons_et_al2014a}),
%this analysis took a slightly different approach which included some critical
%steps to improve our upper limit. In short, the improvements included using a
%new, refined redundant calibration method (Zheng 2014), increasing the width of
%the wideband delay filter that removes smooth spectrum foregrounds, weighting
%the lst binned data sets, and optimal fringe rate filtering. In section
%\ref{sec:analysis}, we dicuss each of the improvements in more detail.
%
%Figure \ref{fig:step_through_pspec} (TBD) shows the power spectra when each of
%the steps mentioned above are turned off and for the one where all of them are
%turned on. We can see the gradual improvement of the power spectra (hopefully).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Instrument Discussion}
\subsection{Instrument Stability}

\begin{figure*}[!t]
\centering
\includegraphics[width=2.3\columnwidth]{plots/density.png}
\caption{Visibility density for a redundant baseline group.}
\label{fig:density}
\end{figure*}

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{plots/stability.png}
\caption{Time stability of the instrument.}
\label{fig:stability}
\end{figure}


One of the key aspects of PAPER is the stability of the instrument between
baselines and in time. Figure \ref{fig:density} shows for a given redundant
group of baselines, the spread in the visibilites for all of the baselines in
that group as a function of LST's. Specifically, we histogram the real part of
the visibilities for all baselines belonging to the East-West 30 m redundant
baseline in a given LST bin for foreground contained data. We see that for a
given LST bin the, spread in the value for all the baselines is ~100 Jy.  We get
more samples per LST bin during the lst range of 2-10 hours due to our observing
season, therefore the density of points in this LST region is greater, set by
the color scale. This density plot shows that redundant baselines, especially
the EoR baselines we are using the power spectrum analyisis, agree very well
with one another. This is another nod to the use of Omnical for redundant
calibration. 

In addition to stability between redundant baselines, we also consider the
stability in time for the PAPER instrument. In order to quantify the stability
in time we extract one channel for a given baseline for every observation day
and bin in LST. We then Fourier transform along the time direction for every LST
bin and compute the power spectrum. The DC bin is the average of all of the days
and clearly there is excess power there. However, for time modes greater than 1
day, we see that there is little correlation between days, showing that there
are no systematics in time. However, we do note that on the 2 day time scale,
there is a slight increase power owning the fact that neighboring days are
slightly correlated with each other.  Figure \ref{fig:stability} shows the
stability of the instrument as a function of time. 

\subsection{System Temperature}   
During the LST binning step, the variance of the visibilities on a frequency and
time basis, are recorded. Using these variances, we calculate the system
temperature as a function of LST, averaging over each LST hour. 
\begin{equation}
    T_{sys} = \frac{T_{rms}}{\sqrt{\Delta\nu t_{int}}}, 
\end{equation}
where $\Delta\nu$ is the bandwidth, $t_{int}$ is the integration time, and
$T_{rms}$ is the RMS temperature.
Figure \ref{fig:tsys} shows the results of this calculation. In this observing
season, the system temperature stays consistent with previous estimates of the
as in \citet{parsons_et_al2014} and \citet{jacobs_et_al2014} at $T_{sys} =
500K$ at 160 MHz.

\begin{figure}[b!]\centering
\includegraphics[width=\columnwidth]{plots/tsys.png}
\caption{System temperature as a function of frequency plotted for multiple LST
bands. $T_{sys}$ is calculated for every integration using the variance
statistic calculated in every LST bin and average on hour time scales to
estimate the $T_{sys}$ reported here. For a majority of the LST bins, especially
those bins used in our analysis, the system temperature is about 500K at 160
MHz, consistent with previous estimates of the system temperature. XXX note that
this has the correction factor applied to it, assuming gaussian statistics.}
\label{fig:tsys}
\end{figure}
Previous estimates of the system temperatue
\citep{parsons_et_al2014,jacobs_et_al2014} relied on differencing and averaging
baselines, time samples, and/or frequency channels. The agreement between these
various methods of estimating the system temperature provides a robust measure
of the system temperature of the PAPER instrument. 


\section{Results and Discussion}
\subsection{Power Spectrum}
Figure \ref{fig:final_pspec} shows the spherically averaged power spectrum,
$P(k)$, we measure at $z=8.4$, as well as the dimensionless power spectrum
$\Delta^{2} = \frac{k^{3}P(k)}{2\pi^{2}}$. These power spectra are incoherently
averaging over the 3 different redundant baselines used in this analysis. The
bootstrapped error bars are derived from the power spectra of all three of the
separations together, not independently. This is because the thermal noise in
the measurements between the three separations are drawn from the same
distribution. The error bars are determined independently for the dimensionless
power spectrum. We determine an upper limit in $\Delta^{2}$ by using most of the
data points in the measured power spectrum. We determine the posterior
distribution of the values in the power spectrum for $.15<k<.5$ to avoid the
points within the horizon and the clear detections at the two modes outside of
the horizon which we argue are foreground contaminated. Figure
\ref{fig:final_posterior} shows the posterior distribution of the power spectrum
by fitting a flat curve in $\Delta^{2}$ to the power spectrum assuming
Gaussian error bars. From this distribution, we determine the 2 sigma upper
limit to be XXX $mk^{2}$, which is consistent with zero.

We also include the power spectrum before foreground filtering for comparison
and to show the effect of the delay filter within the horizon. In addition we
plot the theoretical 2 sigma errors expected for a $T_{sys} = 350 K$. These
error bars were calculated using a full simulation of the sensitivity of the
instrument, with the appropriate weightings due to uneven sampling of LST bins
as described in \cite{pober_et_al2014}. These errors agree well with the
sensitivity calculations done in \cite{parsons_et_al2012a}. 
%XXX need to get this simulation from Jonnie.

Some key features in the power spectrum require some mention and explanation. We
first notice that for the first two modes outside of the horizon, there are
clear detections. These are attributed to residual foreground leakage from
inside the horizon, which arises from imperfections in the model of the beam and
bandpass. We also notice that at $k\approx{.35}\ h\ {\rm Mpc}^{-1}$, as in
\cite{parsons_et_al2014}, that there is a slight excess. This slight excess is
due to some systematic, and most likely foreground emission. The power in this
bin has gone down by almost an order of magnitude as compared the
\cite{parsons_et_al2014}, which we attribute to the optimal fringe rate filter.
Because we are down weighting fringe rate bins that do not correspond to points
low in the beam, the decreased power at $k\approx{.35}\ h\ {\rm Mpc}^{-1}$ is
most likely attributed to this fringe rate filter down weighting sources towards
the horizon. The foreground emission at this high $k$-mode, is likely attributed
to polarization leakage from a high rotation measure source near the horizon.
Finally, we also note that most for most of the $k$-modes, measurements are
consistent with zero. Thus, we are still thermal noise dominated and are still
integrating down. The systematics we are facing are most likely foregrounds that
are spreading power outside of the horizon and instrumental systematics. In
addition our $2\sigma$ upper limit is also consistent with zero and therefore we
are noise dominated and are hence lacking sensitivity.

\subsection{Brightness Temperature Contstraints}
With the new upper limit from the the 21 cm power spectrum in hand, we can put
constraints on the mean brightness temperature of the the 21 cm line. This new
upper limit is pushing the cusp of reionization scenarios, but is still around
an order of magnitude away from the plausible reionization scenarios. Using
various reionization scenarios, we can put upper limits on the mean brightness
temperature of the of the 21 cm transition.

The relative temperature between the cosmic microwave bacground, $T_{\gamma}$,
and the spin temperature, $T_{s}$, is given by 
\begin{equation}\label{eqn:tb}
    \delta{T_{b}}(\nu) = \frac{T_{s} - T_{\gamma}(z)}{1+z}(1-e^{-\tau_{\nu}}).
\end{equation}
When considering a low optical depth medium, \ref{eqn:tb} becomes 
\begin{equation}\label{eqn:tb_approx}
    \delta{T_{b}}(\nu) \approx \frac{T_{s} - T_{\gamma}}{1+z}\tau_{\nu},
\end{equation}
where the optical depth is given by, 
\begin{align}\label{eqn:tau}
    \tau_\nu &= \frac{3c^{3}\hbar A_{10} n_{\text{\tiny{HI}}}}{16k\nu_{o}^{2}T_{s}H(z)} \\
            &\approx 8.6\times10^{-3}(1+\delta)x_{\text{\tiny{HI}}}\Big[\frac{T_{\gamma}(z)}{T_{s}}\Big] \Big(\frac{\Omega_{b}h^{2}}{0.02}\Big)\Big[\Big(\frac{0.15}{\Omega_{m}h^{2}}\Big)\Big(\frac{1+z}{10}\Big)\Big]^{1/2}
\end{align}
ignoring effects due to redshift space distortions. $x_{HI}$ is the neutral
fraction of hydrogen and $\delta$ is the local baryon overdensity. 
Putting it all together and using the most recent Plack results, 
\begin{equation}
    \delta{T_{b}} \approx 25x_{\text{\tiny{HI}}}\Big(\frac{1+z}{10}\Big)^{1/2}(1+\delta)\Big[1 - \frac{T_{\gamma}(z)}{T_{s}}\Big] \,\text{mK}.
\end{equation}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   -Tsys, and Tsys vs. time ?


%
%   -Final Pspecs and from various stages 
%       --before/after omnical: Data in
%           /data2/home/zakiali/psa_live/forlstbinning for the before data set.
%           Currently being binned.
%           Note that this data set has ~271 fewer files ~ 48 hours. Hence, it
%           is not the same as the omnical data set.
%           After data set is the one we have always been using : 
%           /data2/home/zakiali/psa_live/forlstbinning_omnical_2 :
%           lstbin_even and lstbin_odd.
%       --before/after frf. Ditto in the above directories.
%       --before after foregrounds. There is foreground data in there as well.
%       SHould we make a foreground run with the non omnical set as well.

%   -Future datasets. Things to look forward to.
%
%   -HERA
%
%   -Relative merrits of foreground avoidance vs fg filtering.
%
%   -relative importance of improvements of psa32.
%
%   -vamp on consistency with zero.
%
%   -Remaining challenges.
%
%       --Polarization leakage.
%       --Sensitivity 
%       --Seeing foregrounds.
%
%
%
%


%   -Implications for polarized foregrounds.
%       --Flip this around on David and James : We don't see anything at this
%         level, which implies such and such for polarized emission.
%   -Radio recombination and other unsmooth foregrounds. 
%       --paper by peng, Oh. We could maybe confirm this??
%   -Summarize Jonnie's Result.
%   -Repeat the science measurement in Parsons 2014. This will compliment the
%    Jonnies result.
%   -the wedge - maybe?
\begin{figure*}[h!]\centering
\includegraphics[width=1.5\columnwidth,height=1\columnwidth]{plots/pk_k3pk.png}
\caption{The resulting estimate of the 21 cm power spectrum with 2$\sigma$ error
bars. Vertical dashed line at $k\approx{.6\ h\ Mpc^{-1}}$ correspond to the
horizon limit.}
\label{fig:final_pspec}
\end{figure*}

\begin{figure}[h!]\centering
\includegraphics[width=\columnwidth]{plots/flat_k3pk_posterior.png}
\caption{The posteriror distributiom assuming a flat, gaussian power spectrum in
$\Delta^{2}$ (blue). The green and red horizontal lines correspond to the 1 and
2 $\sigma$ upper limits, respectrively, on the power spectrum.}
\label{fig:final_posterioir}
\end{figure}


\subsection{Comparison with PSA32}
[XXX need to rerun power spectrum flux scale fix with the non omnical applied
data and non optimal fringe rate filter]
%   -We now compare and contrast our result with those without some of the key
%   components of our analysis pipeline.
%
%   -First we consider the effect from omnical. 
%       -Applying a more precise calibration scheme increases the redundancy
%       between baselines. Hence, this decrease the noise in the measurement. 
%       -This is exactly what we see in the  final power spectrum measurement. 
%       figure \pspecfigure shows the power spectrum of data that has not had
%       omnical applied to it. THe error bars here are reduced by a factor of a
%       few. 
%
%   -Next we consider the effect of using an optimal fringe rate filter. 
%       -By applying an optimal fringe rate filter, we down weight fringe rate
%       bins that are less sensitive, and hence have less signal, and upweight
%       the more sensitive parts of the sky. 
%       -Figure \ref{fig:frf_pspec} shows the power spectrum with a naive fringe
%       rate filer, which is a flat filter and throws out the the fringe rates
%       outside of those possible for a baseline of the eor baselines used in
%       this analysis. 
%       -This gives us a sensitivity benefit of a factor of 3(XXX), as seen
%       in the figure. Note that this data does have omnical applied to it.
%

\begin{figure*}[t!]\centering
\includegraphics[width=1.5\columnwidth]{plots/pspec_comparison.png}
\caption{The comparison of power spectra with and without various analysis in
the pipeline applied to it. The blue power spectrum has the full pipeline
applied to it and is identical to \ref{fig:final_pspec}. The red power spectrum
does not have omnical applied to it, and hence the error bars are bigger. Note
that it does have the otimal fringe rate filter applied to it. Finally, the
green power sepctrum does not have the optimal fringe rate filter applied.
However, it does have a fringe rate filter applied to that excludes negative
fringe rates and those greater than the maimum fringe rate for the given
baseline (east-west 30 m baselines). }
\label{fig:pspec_comp}
\end{figure*}

Here, we discuss the effect to the power spectrum if various stages of the
analysis were kept out. Specifically, we will discuss the effects of using
Omnical and optimal fringe rate filtering vs. not using them. This will give a
sense of how different analyses effect the power spectrum.

The merits of using omnical were described above and it has the effect of
bringing redundant baselines into better agreement with one another by
calibrating out the differences between them. With this picture, we expect
that for the power spectrum, error bars should become tighter. That is the
noise in the measurement would be reduced. Comparing P($k$) for data that has
been Omnical calibrated vs data that has not, but does have the redundant
calibration that was used in \cite{parsons_et_al2014} applied to it, we see that
the error bars have decreased. This is shown in figure \ref{fig:pspec_nomni}.
Note that the error bars shrink for the higher $k$-modes, the error bars for the
2 modes outside of the horizon do not have this systematic reduction because of
the wideband delay filter. The wideband delay filter is applied after the
calibration solutions have been applied to the visibilities and the 2 modes
outside of the horizon are artifacts of the foregrounds leaking outside of the
horizon. 

Optimal fringe rate filtering is a new technique that we have employed in our
analysis and the effect it has on the power spectrum is crucial, considering it
has a sensitivity benefit of a factor of 2. First of all, this fringe rate
filter improves the SNR by a factor of 2 for PAPER, increasing the total
sensitivity of our measurement. Briefly, the filter does this by up weighting
parts of the sky that are illuminated by the primary beam and down weighting
parts of the sky that contain more noise. This has the effect of de noising the
data. Figure \ref{fig:pspec_comp} shows the effect of minimal fringe rate
filtering. The filter applied here is the same as described in
\cite{parsons_et_al2014}. It evenly weights fringe rates from 0 to $f_{max}$,
where $f_{max}$ is the maximum fringe rate possible for a given baseline and
frequency, discarding negative fringe rates. As can be seen in figure
\ref{fig:pspec_comp}, the power spectrum in $\Delta^{2}$ comes down by a factor of
2-3. The two modes outside of the horizon, which are dominated by foregrounds
and are clear detections, also come down. The fact that these come down, implies
that the foregrounds are down in the beam possibly near the horizon. Therefore,
the signal there is attenuated by the use of this fringe rate filter. 

It is interesting that at $k\approx.25$, we get a detection of something that
wasnt there before. Foregrounds? probably low level systematics we are hitting
up against. Going to take some convincing...

The new techniques and improvements to calibration used in this analysis are a
considerable improvement from the PAPER-32 analysis from
\cite{parsons_et_al2014}. These new techniques bring into light the new ways in
which we can gain sensitivity and accurately describe our instrument. The use of
Omnical to accurately calibrate the relative complex gains of the antennas has
shown to be a major improvement to the pipeline. The accuracy and improvement of
this calibration brings redundant baselines more into agreement with one another
and provides and axis for us to check the status of the array. The output 
chi-squared from Omnical provides a metric for us to monitor the health of the
array and another axis for us to flag off bad RFI events. Furthermore, the
precise calbration of antennas is imperative to accurately model and remove
foregrounds. Using the delay spectrum technique employed here, precise
calibration is necessary to mitigate the leakage of foreground power to modes
outside of the horizon for a given baseline. 

\subsection{Remaining Challanges}
The closer we get to a detection of the 21 cm EoR fluctuations, the more we need
to know what the systematics can be and at what level they can come in at. One
of the biggest challanges remaining, as metioned above, is the limited
sensitivity of current EoR experiments. Even with the full sensitivity of the
PAPER array at full build out, there will only be a $1.65\sigma$ EoR detection
with the current method of foreground avoidence to detect the power spectrum.
With the complete optimist approach of working withing the wedge, there will be
a $8.86\sigma$ detection of the 21 cm power spectrum \citep{pober_et_al2014}.
However, the configuration of PAPER makes it difficult to localize sources for
removal and makes it hard to work within the wedge. These sensitivity benefits
are being addressed by second generation EoR experiments, whose goal is to
characterize the 21 cm power spectrum at high redshifts.

In addition to sensitivity limitations, foregrounds are also a challange that
needs to be met. The delay spectrum approach requires that foregrounds need to
be spectrally smooth so that they are localized to inside the horizon. As seen
from equation \ref{eqn:delay_filter}, this approach also requires the need to
know your beam and bandpass very well to mitigate the effect of spreading power
outside the horizon. The PAPER beam is both spaitially and spectrally smooth
\cite{pober_et_al2012}, but even small variations have the effect of
making sppilling power outside of the horizon. In addition to the beam, the
major source of error is in the bandpass. The 9th order polynomial fit to the
bandpass is strictly not just the bandpass but contains ripples imprinted by
sources other than Pictor A. Therefore characterizing the true bandpass of the
instruement is hard and largely unknown for PAPER.

Finally, one of the possibly biggest challenges we face is that of polarization
leakage due to Faraday rotation of polarized sources from Stokes Q into Stokes
I \citep{jelic_et_al2010,jelic_et_al2014}. 
Through simulations, \cite{moore_et_al2013} showed that the expected
polarization leakage at $z=8.5$ in $\Delta^{2}$ is between
$10^{3}-3\times10^{3} (mk^{2})$ at $k\approx{.15}hMpc^{-1}$. 
[XXX careful with this comparison, since it is about to be revised downward in new moore et al paper]
Our measured Stokes
I power spectrum in figure \ref{fig:final_pspec} shows that we just on the cusp
of seeing polarization leakage and possibly even ruling out some of the models
used. [XXX i don't think this statement is backed up by the data]
There might even be possible detection of the polarization leakage and
would require us to move up in frequency to higher redshift where polarization
is less prominent due to the $\lambda^{2}$ dependence of Faraday rotation.
However, if we were seeing polarization leakage at $k\approx{.15}hMpc^{-1}$, we
would expect to see leakage at the higher $k_{\parallel}$ in our power spectrum
since polarization leakage grows monotonically in $k$, for the modes we measure.
We can conclude the this detection at $k=.15$ is just foreground leakage from
within the horizon or some other systematics.

For more sensitive arrays, like HERA and the SKA, polarization leakage may play
a big problem and would require mitigation contingencies. One of the main ways
that polarization leakage occurs is from elliptical beams, and hence a mismatch
between the X and Y beams (for a crossed linear dipole design). Therefore, one
way of mitigating the effects of polarization leakage is to design the
instrument such that the beams are more circularly symmetric and therefore the
mismatch between the X and Y polarizations is minimized. For PAPER this mismatch
is expected to be at roughly $10\%$, which was used in the simulations in
\cite{moore_et_al2013}.  The EoR signal is expected to be in the 10's of mK, and
since these second generation instruments have the ability to make detect EoR
with very significant confidence, the need to mitigate the polarization leakage
effects, which may be up to 2 orders of magnitude brighter than the EoR signal,
is a necessity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
We have shown a new upper limit on the 21 cm EoR power spectrum at $z=8.4$,
showing an almost an order of magnitude of improvement from the previous result
\cite{parsons_et_al2014}. We found the $2\sigma$ upper limit of XXX by fitting a
flat power spectrum in a $k$ range from $.15<$k$<.5\ h\ {\rm Mpc}^{-1}$ to the
dimensionless power spectrum, $\Delta^{2}$. Using these limits we have improved
the limits on the 21 cm mean brightness temperature set in
\cite{parsons_et_al2014}.

We also continue to demonstrate the efficacy of using delay filters for
foreground suppression and the foreground avoidance technique in measuring the power
spectrum. In addition, we employ novel techniques in this analysis and show that
the necessity of precise calibration is needed to make accurate and believable
measurements of the 21 cm EoR power spectrum. We also show that the use of beam
weighted measurements in the optimal fringe rate filter is critical to
increasing sensitivity for instruments with natively low sensitivity due to
collecting area, increasing the SNR on the measurement by a factor of atleast 2.
We have also demonstrated the stability of the PAPER instrument both in time and
between redundant baselines, showing that baselines agree on the same sky up to
systematical errors and that the time correlation between days is low in the
noise.

At the acheived levels of the power spectrum, we also put constraints on the
amount polarization leakage that could possibly corrupt the Stokes I power
spectrum. We show that polarization leakage is not shown for most $k$-modes at
the level of $\sim10^{3}\ \text{mK}^{2}$. 

Furthermore, the analysis in this paper is extended in (cite pobers science
paper) by showing the science implications of this upper limit. They show that
given this upper limit, we can rule out regions of spin temperature, $T_{s}$,
and neutral fraction, $x$, that are not consistent with this result. In
addition, they compare this result with current X-ray lumiosity functions and
show that there are not enough X-ray sources to cause heating of the IGM at
$z=8.4$ that are consistent with these results.

Looking forward, the need for larger arrays with increased collecting area is
becoming a necessicity. Proposed experiments such as the Square-Kilometer Array
(SKA) and the Hydrogen Epoch of Reionization Array (HERA) are set to fill this
need. HERA is a collaborative experiment dedicated to detect and characterize
the 21 cm EoR power spectrum as well as eventually image the ionizing bubbles
from the formation of the first stars and galaxies. The HERA instrument
is proposed to be composed of a closely packed 547 element hexagonal array 
\citep{pober_et_al2014} with each element being a 14-m diamter
reflecting-dish, with a total array area of $\sim.1km^{2}$. Sensitivity forcasts
derived in \citet{pober_et_al2014} showed that HERA has the ability to make a
detection of EoR to a significance from $\sim32\sigma$ to $\sim133\sigma$
depending on the foreground removal scheme used. The lower bound is for the
delay spectrum technique used in this paper, where modes within the horizon are
irrevocably corrupted, whereas the higher bound equates to when we can perfectly
work within the the wedge. That is, it is possible to perfectly subtract sources
within the horizon.

PAPER itself is looking forward to 2 seasons of 128 antenna maximally redundant
configuration data set. This data set has to potential to increase  sensitivity
by a factor of $3.5$ as compared to the data set used in this paper. However,
due to constraints on the PAPER site in South Africa, the shortest baselines for
this array are $8\lambda$ (16m), instead of the $16\lambda$ (32m) baselines used
in this analysis, which may be riddled with systematics due to cross coupling
and reflections whithin the array. Therefore, only considering the 16 m
East-West baselines, there will be a total sensitivity benefit of $3$. However,
there is the potential to use the $48m$ nd $64m$ baselines in this analysis as
well, boosting the sensitivity to by another factor of 2. 

PAPER is continuing to integrate down on the 21 cm EoR signal and gaining ground
on overcoming challanges in systematics in the instrument as well being able to
look past foregrounds. Future work will again characterize the EoR power
spectrum over a range of redshifts and to measure directly the polarized power
spectrum. We also look forward to two seasons of 128 dual polarization PAPER
data which will increase the sensitivity of measurements by almost an order of
magnitude from the data used here. Also, construction of HERA is also under way,
which will have the sensitiviy to make a $n\sigma$ detection of the 21 cm power
spectrum. We look forward to this.


%However, all major 21 cm experiments (LOFAR, MWA, and PAPER) are at the cusp of detection
%and all have the sensitivity to rule out the optimistic reionization scenarios.
% XXX don't put speculative stuff in intro
XXX Might mention impact of wedge on predicted sensitivities.
However, even with a detection from one of these experiments, a high signal-to-noise characterization of the 21 cm power spectrum and imaging of EoR will have to
wait until future 21 cm experiments such as the Hydrogen Epoch of Reionization
Array (HERA) and the Square Kilometer Array (SKA), which will have more raw
collecting area and greater sensitivity \citep{pober_et_al2014}.

\section{Acknowledgements} 

PAPER is supported by grants from the National Science Foundation (NSF; awards 0804508,
1129258, and 1125558).  ARP, JCP, and DCJ would like to acknowledge NSF support
(awards 1352519, 1302774, and 1401708, respectively).
JEA would like to acknowledge a generous grant from the Mount Cuba Astronomical Association for
computing resources.
We graciously thank SKA-SA for onsite infrastructure and observing support. In
addition we would like to thank our South African interns Monde Manzini and
Ruvano Casper from Durban University of Technology (DUT), who helped build out
the array from 32 antennas to the 64 antennas this analaysis was based on. 
We would like to thank Josh Dillion for helpful discussions on optimal quadratic
estimators. 
[XXX ask collaborators for other acknowledgements].

%\clearpage
%\nocite{*}
\bibliographystyle{apj}
\bibliography{biblio}

\end{document}

